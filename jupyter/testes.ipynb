{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6700796d",
   "metadata": {},
   "source": [
    "# üéØ **NOTEBOOK ULTRA-DOCUMENTADO - SEGMENTA√á√ÉO SEM√ÇNTICA**\n",
    "\n",
    "---\n",
    "\n",
    "## üìö **SISTEMA COMPLETAMENTE COMENTADO E DID√ÅTICO**\n",
    "\n",
    "### ‚úÖ **Este notebook foi super-detalhado para ensino e aprendizado:**\n",
    "- üîç **Cada linha de c√≥digo explicada** em detalhes\n",
    "- üìä **Conceitos matem√°ticos** fundamentados\n",
    "- üéØ **Aplica√ß√µes pr√°ticas** de cada t√©cnica\n",
    "- üí° **Dicas de otimiza√ß√£o** e ajuste fino\n",
    "- ‚ö†Ô∏è **Armadilhas comuns** e como evit√°-las\n",
    "\n",
    "---\n",
    "\n",
    "## üèóÔ∏è **ARQUITETURA DO SISTEMA:**\n",
    "\n",
    "### üì¶ **1. SETUP E CONFIGURA√á√ÉO (C√©lulas 1-3):**\n",
    "- **C√©lula 1**: Imports detalhados com prop√≥sito de cada biblioteca\n",
    "- **C√©lula 2**: Configura√ß√µes base explicadas (IMG_SIZE, EPOCHS, etc.)\n",
    "- **C√©lula 3**: Par√¢metros avan√ßados com guides de ajuste\n",
    "\n",
    "### üéØ **2. LOSS FUNCTIONS E M√âTRICAS (C√©lula 4):**\n",
    "- **Dice Loss**: Matem√°tica + aplica√ß√£o para bordas precisas\n",
    "- **Focal Loss**: Solu√ß√£o completa para classes desbalanceadas\n",
    "- **Combined Loss**: Estrat√©gia h√≠brida otimizada\n",
    "- **IoU Metrics**: Avalia√ß√£o padr√£o ouro explicada\n",
    "\n",
    "### üèóÔ∏è **3. ARQUITETURAS U-NET (C√©lula 5):**\n",
    "- **U-Net Base**: Implementa√ß√£o cl√°ssica comentada\n",
    "- **U-Net Improved**: T√©cnicas modernas (BatchNorm, Dropout)\n",
    "- **U-Net Attention**: Estado da arte com Attention Gates\n",
    "\n",
    "### üé® **4. DATA AUGMENTATION (C√©lula 6):**\n",
    "- **B√°sico vs Avan√ßado**: Quando usar cada um\n",
    "- **Sincroniza√ß√£o**: Como manter imagem e mask alinhadas\n",
    "- **Par√¢metros √≥timos**: Para diferentes cen√°rios\n",
    "\n",
    "### ‚öôÔ∏è **5. CALLBACKS E OTIMIZA√á√ÉO (C√©lula 7):**\n",
    "- **EarlyStopping**: Preven√ß√£o de overfitting\n",
    "- **ReduceLR**: Ajuste autom√°tico de learning rate\n",
    "- **Learning Rate Schedulers**: Diferentes estrat√©gias\n",
    "- **Otimizadores**: Adam, AdamW, SGD comparados\n",
    "\n",
    "### üöÄ **6. SISTEMA INTEGRADO (C√©lulas 8-11):**\n",
    "- **Treinamento Modular**: Fun√ß√£o completa automatizada\n",
    "- **Visualiza√ß√£o Avan√ßada**: Plots de m√©tricas e predi√ß√µes\n",
    "- **Compara√ß√£o de Arquiteturas**: Benchmark autom√°tico\n",
    "- **An√°lise Final**: Relat√≥rios detalhados\n",
    "\n",
    "---\n",
    "\n",
    "## üí° **COMO USAR ESTE NOTEBOOK:**\n",
    "\n",
    "### üéì **Para Aprendizado:**\n",
    "1. **Leia os coment√°rios** antes de executar cada c√©lula\n",
    "2. **Experimente par√¢metros** diferentes nas configura√ß√µes\n",
    "3. **Execute c√©lula por c√©lula** para entender o fluxo\n",
    "4. **Modifique valores** e observe os resultados\n",
    "\n",
    "### üöÄ **Para Produ√ß√£o:**\n",
    "1. **Execute todas as c√©lulas** em sequ√™ncia\n",
    "2. **Ajuste CONFIG** para seus dados espec√≠ficos\n",
    "3. **Monitore m√©tricas** durante o treinamento\n",
    "4. **Use modelo salvo** para infer√™ncia\n",
    "\n",
    "### üß™ **Para Experimenta√ß√£o:**\n",
    "1. **Modifique LOSS_PARAMS** para testar different loss functions\n",
    "2. **Teste ARCHITECTURES** diferentes para comparar\n",
    "3. **Ajuste AUGMENTATION_PARAMS** para seus dados\n",
    "4. **Use callbacks** para otimiza√ß√£o autom√°tica\n",
    "\n",
    "---\n",
    "\n",
    "## üîß **CONFIGURA√á√ïES R√ÅPIDAS:**\n",
    "\n",
    "### ‚ö° **Teste R√°pido (5 minutos):**\n",
    "```python\n",
    "CONFIG['EPOCHS'] = 5\n",
    "CONFIG['BATCH_SIZE'] = 8\n",
    "AUGMENTATION_PARAMS['rotation_range'] = 15\n",
    "```\n",
    "\n",
    "### üéØ **Produ√ß√£o Completa (algumas horas):**\n",
    "```python\n",
    "CONFIG['EPOCHS'] = 500  # Com EarlyStopping\n",
    "CONFIG['BATCH_SIZE'] = 16\n",
    "# Use configura√ß√µes padr√£o otimizadas\n",
    "```\n",
    "\n",
    "### üî¨ **Experimenta√ß√£o Avan√ßada:**\n",
    "```python\n",
    "# Teste diferentes combina√ß√µes de loss\n",
    "LOSS_PARAMS['loss_weights'] = [0.3, 0.5, 0.2]\n",
    "# Ou teste arquiteturas espec√≠ficas\n",
    "architecture='unet_attention'\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**üéä NOTEBOOK PRONTO PARA USO DID√ÅTICO E PROFISSIONAL! üéä**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bc2dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Carregando bibliotecas...\n",
      "‚úÖ Todas as bibliotecas carregadas!\n",
      "üîß TensorFlow: 2.19.0\n",
      "üî¢ NumPy: 2.1.3\n"
     ]
    }
   ],
   "source": [
    "# üì¶ IMPORTS E DEPEND√äNCIAS\n",
    "# =====================================\n",
    "# Esta c√©lula carrega todas as bibliotecas necess√°rias para o projeto\n",
    "# Cada import tem um prop√≥sito espec√≠fico no pipeline de segmenta√ß√£o\n",
    "# =====================================\n",
    "\n",
    "print(\"üì¶ Carregando bibliotecas...\")\n",
    "\n",
    "# TENSORFLOW & KERAS - Framework principal de Deep Learning\n",
    "import tensorflow as tf              # Framework de ML com suporte a GPU, base do projeto\n",
    "import numpy as np                   # Manipula√ß√£o eficiente de arrays num√©ricos e opera√ß√µes matem√°ticas\n",
    "import matplotlib.pyplot as plt      # Visualiza√ß√£o de imagens, gr√°ficos de training e resultados\n",
    "import seaborn as sns               # Visualiza√ß√µes estat√≠sticas avan√ßadas (heatmaps, distribui√ß√µes)\n",
    "\n",
    "# SCIKIT-LEARN - Ferramentas de avalia√ß√£o e m√©tricas\n",
    "from sklearn.metrics import confusion_matrix, classification_report  # M√©tricas detalhadas de classifica√ß√£o\n",
    "\n",
    "# OPENCV - Processamento de imagens\n",
    "import cv2                          # Redimensionamento, transforma√ß√µes e manipula√ß√£o de imagens\n",
    "\n",
    "# KERAS LAYERS - Componentes da rede neural\n",
    "from tensorflow.keras.layers import *    # Todas as camadas: Conv2D, MaxPooling2D, BatchNormalization, etc.\n",
    "\n",
    "# KERAS MODELS - Constru√ß√£o e gerenciamento do modelo\n",
    "from tensorflow.keras.models import Model   # Classe para cria√ß√£o de modelos funcionais personalizados\n",
    "\n",
    "# KERAS OPTIMIZERS - Algoritmos de otimiza√ß√£o\n",
    "from tensorflow.keras.optimizers import Adam   # Otimizador Adam (padr√£o para deep learning)\n",
    "\n",
    "# KERAS CALLBACKS - Funcionalidades durante treinamento\n",
    "from tensorflow.keras.callbacks import *   # EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, etc.\n",
    "\n",
    "# KERAS PREPROCESSING - Data augmentation e gera√ß√£o de dados\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator   # Augmenta√ß√£o de dados em tempo real\n",
    "\n",
    "print(\"‚úÖ Todas as bibliotecas carregadas!\")\n",
    "print(f\"üîß TensorFlow: {tf.__version__}\")\n",
    "print(f\"üî¢ NumPy: {np.__version__}\")\n",
    "\n",
    "# üí° INFORMA√á√ïES IMPORTANTES:\n",
    "# ‚Ä¢ TensorFlow: Detecta automaticamente GPU se dispon√≠vel\n",
    "# ‚Ä¢ NumPy: Todas as imagens s√£o convertidas para arrays NumPy\n",
    "# ‚Ä¢ Matplotlib: Usado para visualizar resultados e debugging\n",
    "# ‚Ä¢ OpenCV: Eficiente para redimensionamento e transforma√ß√µes de imagem\n",
    "# ‚Ä¢ Keras: API de alto n√≠vel do TensorFlow, mais simples de usar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac15ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è Configurando par√¢metros base...\n",
      "‚úÖ Configura√ß√µes base definidas!\n",
      "üìä Imagem: (256, 256)\n",
      "üîß Batch: 16, LR: 0.0001\n",
      "üéØ Classes: 3 - ['Background', 'Cat', 'Dog']\n"
     ]
    }
   ],
   "source": [
    "# ‚öôÔ∏è CONFIGURA√á√ïES BASE DO PROJETO\n",
    "# =====================================\n",
    "# Par√¢metros fundamentais que controlam todo o comportamento do modelo\n",
    "# Altere estes valores para experimentar diferentes configura√ß√µes\n",
    "# =====================================\n",
    "\n",
    "print(\"‚öôÔ∏è Configurando par√¢metros base...\")\n",
    "\n",
    "# DICION√ÅRIO DE CONFIGURA√á√ïES CENTRALIZADAS\n",
    "CONFIG = {\n",
    "    # üñºÔ∏è DIMENS√ïES DAS IMAGENS\n",
    "    'IMG_SIZE': (256, 256),         # Resolu√ß√£o de entrada: (altura, largura)\n",
    "                                    # 256x256: balan√ßo entre qualidade e velocidade\n",
    "                                    # Maior = mais detalhes, mas mais lento\n",
    "                                    # Menor = mais r√°pido, mas menos precis√£o\n",
    "    \n",
    "    # üìä PAR√ÇMETROS DE TREINAMENTO\n",
    "    'BATCH_SIZE': 16,               # Quantas imagens processadas simultaneamente\n",
    "                                    # 16: padr√£o para GPUs de 8-16GB VRAM\n",
    "                                    # Menor = menos mem√≥ria, converg√™ncia mais est√°vel\n",
    "                                    # Maior = mais eficiente, mas requer mais VRAM\n",
    "    \n",
    "    'EPOCHS': 500,                  # üî• N√öMERO M√ÅXIMO DE √âPOCAS DE TREINAMENTO\n",
    "                                    # 500: valor alto, pois usamos EarlyStopping\n",
    "                                    # O treinamento para quando n√£o h√° melhoria\n",
    "                                    # Para testes r√°pidos: 10-50 √©pocas\n",
    "                                    # Para resultados finais: 200-500 √©pocas\n",
    "    \n",
    "    'LEARNING_RATE': 1e-4,          # Taxa de aprendizado inicial (0.0001)\n",
    "                                    # 1e-4: padr√£o seguro para Adam optimizer\n",
    "                                    # Maior = converg√™ncia mais r√°pida, risco de instabilidade\n",
    "                                    # Menor = converg√™ncia mais lenta, mas est√°vel\n",
    "    \n",
    "    # üéØ CONFIGURA√á√ïES DO PROBLEMA DE SEGMENTA√á√ÉO\n",
    "    'NUM_CLASSES': 3,               # N√∫mero de classes para segmentar\n",
    "                                    # 3 classes: Background (fundo), Cat (gato), Dog (cachorro)\n",
    "                                    # Esta √© a dimens√£o da sa√≠da final do modelo\n",
    "    \n",
    "    'CLASS_NAMES': ['Background', 'Cat', 'Dog']  # Nomes das classes para visualiza√ß√£o\n",
    "                                    # Ordem IMPORTANTE: deve corresponder aos √≠ndices das masks\n",
    "                                    # 0=Background, 1=Cat, 2=Dog\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Configura√ß√µes base definidas!\")\n",
    "print(f\"üìä Imagem: {CONFIG['IMG_SIZE']}\")\n",
    "print(f\"üîß Batch: {CONFIG['BATCH_SIZE']}, LR: {CONFIG['LEARNING_RATE']}\")\n",
    "print(f\"üéØ Classes: {CONFIG['NUM_CLASSES']} - {CONFIG['CLASS_NAMES']}\")\n",
    "\n",
    "# üí° DICAS PARA ALTERAR CONFIGURA√á√ïES:\n",
    "print(\"\\nüí° GUIA DE AJUSTES:\")\n",
    "print(\"üñºÔ∏è  IMG_SIZE: (128,128) = r√°pido | (256,256) = padr√£o | (512,512) = alta qualidade\")\n",
    "print(\"üìä BATCH_SIZE: 8 = pouca VRAM | 16 = padr√£o | 32 = muita VRAM\")\n",
    "print(\"üî• EPOCHS: 10 = teste r√°pido | 100 = padr√£o | 500 = resultado final\")\n",
    "print(\"‚ö° LEARNING_RATE: 1e-3 = agressivo | 1e-4 = padr√£o | 1e-5 = conservador\")\n",
    "\n",
    "# ‚ö†Ô∏è IMPORTANTE PARA EPOCHS:\n",
    "print(\"\\n‚ö†Ô∏è  NOTA SOBRE EPOCHS:\")\n",
    "print(\"‚Ä¢ O valor 500 √© um m√°ximo - o treinamento para automaticamente\")\n",
    "print(\"‚Ä¢ EarlyStopping monitora val_mean_iou e para quando n√£o melhora\")\n",
    "print(\"‚Ä¢ Para alterar √©poca: modifique CONFIG['EPOCHS'] nesta c√©lula\")\n",
    "print(\"‚Ä¢ Para testes: use 10-50 √©pocas\")\n",
    "print(\"‚Ä¢ Para produ√ß√£o: use 200-500 √©pocas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c4b729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéõÔ∏è Configurando par√¢metros de algoritmos...\n",
      "‚úÖ Par√¢metros de algoritmos configurados!\n",
      "üéØ Loss weights: [0.4, 0.4, 0.2]\n",
      "üî• Focal: Œ±=0.8, Œ≥=2.0\n",
      "üé® Augmentation: rotation=25¬∞\n",
      "‚è∞ Callbacks: EarlyStopping=10, ReduceLR=4\n"
     ]
    }
   ],
   "source": [
    "# üéõÔ∏è PAR√ÇMETROS ESPEC√çFICOS DE ALGORITMOS\n",
    "# =====================================\n",
    "# Configura√ß√µes avan√ßadas para fine-tuning dos algoritmos\n",
    "# Cada par√¢metro foi otimizado para o problema de segmenta√ß√£o de pets\n",
    "# =====================================\n",
    "\n",
    "print(\"üéõÔ∏è Configurando par√¢metros de algoritmos...\")\n",
    "\n",
    "# üéØ PAR√ÇMETROS DAS LOSS FUNCTIONS\n",
    "LOSS_PARAMS = {\n",
    "    # DICE LOSS - Para medir sobreposi√ß√£o entre regi√µes\n",
    "    'dice_smooth': 1e-6,            # Smoothing para evitar divis√£o por zero\n",
    "                                    # Valor pequeno (1e-6) previne instabilidade num√©rica\n",
    "                                    # Menor = mais sens√≠vel | Maior = mais est√°vel\n",
    "    \n",
    "    # FOCAL LOSS - Para lidar com classes desbalanceadas\n",
    "    'focal_alpha': 0.8,             # Peso para classes positivas vs negativas\n",
    "                                    # 0.8: favorece ligeiramente classes positivas (Cat/Dog)\n",
    "                                    # 0.5 = balanceado | >0.5 = favorece positivas\n",
    "    \n",
    "    'focal_gamma': 2.0,             # Fator de foco em exemplos dif√≠ceis\n",
    "                                    # 2.0: padr√£o da literatura, foca em casos dif√≠ceis\n",
    "                                    # 0 = sem foco | 1-3 = foco moderado | >3 = foco extremo\n",
    "    \n",
    "    # COMBINED LOSS - Pesos para combina√ß√£o h√≠brida\n",
    "    'loss_weights': [0.4, 0.4, 0.2]  # [CrossEntropy, Dice, Focal]\n",
    "                                    # 0.4 CCE: estabilidade base\n",
    "                                    # 0.4 Dice: foco na sobreposi√ß√£o de regi√µes\n",
    "                                    # 0.2 Focal: tratamento de desbalanceamento\n",
    "                                    # Soma deve ser 1.0 para manter escala\n",
    "}\n",
    "\n",
    "# üé® PAR√ÇMETROS DE DATA AUGMENTATION\n",
    "AUGMENTATION_PARAMS = {\n",
    "    # TRANSFORMA√á√ïES GEOM√âTRICAS\n",
    "    'rotation_range': 25,           # Rota√ß√£o aleat√≥ria em graus (-25¬∞ a +25¬∞)\n",
    "                                    # 25¬∞: natural para pets, evita rota√ß√µes extremas\n",
    "    \n",
    "    'width_shift_range': 0.15,      # Deslocamento horizontal (15% da largura)\n",
    "    'height_shift_range': 0.15,     # Deslocamento vertical (15% da altura)\n",
    "                                    # 0.15: movimento natural, simula varia√ß√£o de pose\n",
    "    \n",
    "    'shear_range': 0.15,            # Cisalhamento/inclina√ß√£o (15%)\n",
    "                                    # Simula diferentes √¢ngulos de c√¢mera\n",
    "    \n",
    "    'zoom_range': 0.15,             # Zoom in/out (15%)\n",
    "                                    # Simula diferentes dist√¢ncias da c√¢mera\n",
    "    \n",
    "    # TRANSFORMA√á√ïES DE ESPELHAMENTO\n",
    "    'horizontal_flip': True,        # Espelhamento horizontal\n",
    "                                    # True: pets podem aparecer de ambos os lados\n",
    "    \n",
    "    'vertical_flip': True,          # Espelhamento vertical\n",
    "                                    # True: adiciona varia√ß√£o, mas cuidado com realismo\n",
    "    \n",
    "    # TRANSFORMA√á√ïES DE COR/BRILHO\n",
    "    'brightness_range': [0.8, 1.2], # Varia√ß√£o de brilho (80% a 120%)\n",
    "                                    # Simula diferentes condi√ß√µes de ilumina√ß√£o\n",
    "    \n",
    "    'channel_shift_range': 20,      # Varia√ß√£o nos canais de cor\n",
    "                                    # 20: mudan√ßa sutil nas cores, simula diferentes balan√ßos\n",
    "    \n",
    "    'fill_mode': 'reflect'          # Como preencher pixels criados por transforma√ß√µes\n",
    "                                    # 'reflect': espelha bordas, mais natural que zeros\n",
    "}\n",
    "\n",
    "# ‚è∞ PAR√ÇMETROS DE CALLBACKS E OTIMIZA√á√ÉO\n",
    "CALLBACK_PARAMS = {\n",
    "    # REDUCE LEARNING RATE ON PLATEAU\n",
    "    'lr_patience': 4,               # √âpocas sem melhoria antes de reduzir LR\n",
    "                                    # 4: paci√™ncia moderada, evita redu√ß√£o prematura\n",
    "    \n",
    "    'lr_factor': 0.5,               # Fator de redu√ß√£o do learning rate\n",
    "                                    # 0.5: reduz pela metade (conservador)\n",
    "    \n",
    "    'lr_min': 1e-7,                 # Learning rate m√≠nimo\n",
    "                                    # 1e-7: muito pequeno, praticamente para o treinamento\n",
    "    \n",
    "    # EARLY STOPPING\n",
    "    'early_stopping_patience': 10,  # √âpocas sem melhoria antes de parar\n",
    "                                    # 10: paci√™ncia alta, permite recupera√ß√£o de overfitting\n",
    "    \n",
    "    'monitor_metric': 'val_mean_iou', # M√©trica para monitorar\n",
    "                                    # val_mean_iou: m√©trica principal para segmenta√ß√£o\n",
    "    \n",
    "    # MODEL CHECKPOINT\n",
    "    'model_save_path': 'modelo_otimizado_modular.h5'  # Caminho para salvar melhor modelo\n",
    "                                    # Salva automaticamente o melhor modelo\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Par√¢metros de algoritmos configurados!\")\n",
    "print(f\"üéØ Loss weights: {LOSS_PARAMS['loss_weights']}\")\n",
    "print(f\"üî• Focal: Œ±={LOSS_PARAMS['focal_alpha']}, Œ≥={LOSS_PARAMS['focal_gamma']}\")\n",
    "print(f\"üé® Augmentation: rotation={AUGMENTATION_PARAMS['rotation_range']}¬∞\")\n",
    "print(f\"‚è∞ Callbacks: EarlyStopping={CALLBACK_PARAMS['early_stopping_patience']}, ReduceLR={CALLBACK_PARAMS['lr_patience']}\")\n",
    "\n",
    "# üîß DICAS DE AJUSTE POR CEN√ÅRIO:\n",
    "print(\"\\nüîß GUIAS DE AJUSTE:\")\n",
    "print(\"üèÉ TREINAMENTO R√ÅPIDO:\")\n",
    "print(\"  ‚Ä¢ lr_patience=2, early_stopping_patience=5\")\n",
    "print(\"  ‚Ä¢ rotation_range=15, zoom_range=0.1\")\n",
    "\n",
    "print(\"\\nüéØ M√ÅXIMA PRECIS√ÉO:\")\n",
    "print(\"  ‚Ä¢ lr_patience=6, early_stopping_patience=15\")  \n",
    "print(\"  ‚Ä¢ rotation_range=30, zoom_range=0.2\")\n",
    "\n",
    "print(\"\\n‚öñÔ∏è DADOS DESBALANCEADOS:\")\n",
    "print(\"  ‚Ä¢ focal_alpha=0.9, focal_gamma=3.0\")\n",
    "print(\"  ‚Ä¢ loss_weights=[0.3, 0.5, 0.2]\")  # Mais peso no Dice\n",
    "\n",
    "print(\"\\nüñºÔ∏è IMAGENS PEQUENAS/DETALHES:\")\n",
    "print(\"  ‚Ä¢ dice_smooth=1e-7 (mais sens√≠vel)\")\n",
    "print(\"  ‚Ä¢ zoom_range=0.05 (menos zoom)\")\n",
    "\n",
    "# ‚ö†Ô∏è REGRAS IMPORTANTES:\n",
    "print(\"\\n‚ö†Ô∏è REGRAS IMPORTANTES:\")\n",
    "print(\"‚Ä¢ loss_weights deve somar 1.0\")\n",
    "print(\"‚Ä¢ focal_alpha entre 0.1-0.9 funciona melhor\")\n",
    "print(\"‚Ä¢ augmentation muito agressivo pode prejudicar\")\n",
    "print(\"‚Ä¢ early_stopping_patience > lr_patience (recomendado)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e954b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Implementando sistema completo de Loss e M√©tricas...\n",
      "‚úÖ Sistema completo de Loss e M√©tricas implementado!\n",
      "üé≤ Dice Loss: Mede sobreposi√ß√£o, bom para classes desbalanceadas\n",
      "üî• Focal Loss: Foca em exemplos dif√≠ceis, resolve classe dominante\n",
      "‚öñÔ∏è Combined Loss: H√≠brida otimizada (CCE + Dice + Focal)\n",
      "üìè Mean IoU: M√©trica padr√£o para segmenta√ß√£o\n",
      "üîç IoU per Class: An√°lise detalhada por classe\n",
      "üéØ Loss principal: combined_loss_modular\n",
      "‚öñÔ∏è Pesos: CCE=0.4, Dice=0.4, Focal=0.2\n",
      "üî• Focal: Œ±=0.8, Œ≥=2.0\n",
      "üé≤ Dice smoothing: 1e-06\n"
     ]
    }
   ],
   "source": [
    "# üéØ M√ìDULO COMPLETO: LOSS FUNCTIONS E M√âTRICAS\n",
    "# =====================================\n",
    "# Sistema completo de fun√ß√µes de perda e m√©tricas para segmenta√ß√£o sem√¢ntica\n",
    "# Implementa as tr√™s loss functions mais eficazes: Dice, Focal e Combined\n",
    "# =====================================\n",
    "\n",
    "print(\"üéØ Implementando sistema completo de Loss e M√©tricas...\")\n",
    "\n",
    "def dice_loss_modular(y_true, y_pred, smooth=None):\n",
    "    \"\"\"\n",
    "    üé≤ DICE LOSS - Medida de sobreposi√ß√£o entre regi√µes\n",
    "    \n",
    "    =====================================================\n",
    "    üìä CONCEITO MATEM√ÅTICO:\n",
    "    ‚Ä¢ Dice Coefficient = 2 * |A ‚à© B| / (|A| + |B|)\n",
    "    ‚Ä¢ Mede sobreposi√ß√£o direta entre predi√ß√£o e ground truth\n",
    "    ‚Ä¢ Loss = 1 - Dice Coefficient (para minimiza√ß√£o)\n",
    "    \n",
    "    üéØ POR QUE √â EFICAZ PARA SEGMENTA√á√ÉO:\n",
    "    ‚Ä¢ Foca diretamente na sobreposi√ß√£o de pixels\n",
    "    ‚Ä¢ Naturalmente balanceada para classes pequenas\n",
    "    ‚Ä¢ Funciona bem quando regi√µes s√£o pequenas/esparsas\n",
    "    ‚Ä¢ Especialmente boa para bordas e contornos precisos\n",
    "    \n",
    "    üìà CARACTER√çSTICAS:\n",
    "    ‚Ä¢ Range: [0,1] onde 0 = perfeita sobreposi√ß√£o\n",
    "    ‚Ä¢ Smooth: previne divis√£o por zero quando regi√£o est√° ausente\n",
    "    ‚Ä¢ Calculada por classe e depois m√©dia\n",
    "    =====================================================\n",
    "    \"\"\"\n",
    "    if smooth is None:\n",
    "        smooth = LOSS_PARAMS['dice_smooth']  # Usa valor da configura√ß√£o\n",
    "    \n",
    "    # üîß PREPARA√á√ÉO DOS TENSORES\n",
    "    # Converter para formato flat para c√°lculo mais eficiente\n",
    "    y_true_f = tf.cast(tf.reshape(y_true, [-1]), tf.float32)  # Ground truth achatado\n",
    "    y_pred_f = tf.reshape(y_pred, [-1, tf.shape(y_pred)[-1]]) # Predi√ß√µes achatadas [pixels, classes]\n",
    "    \n",
    "    # üéØ CONVERS√ÉO PARA ONE-HOT\n",
    "    # y_true vem como √≠ndices de classe, precisamos converter para one-hot\n",
    "    y_true_one_hot = tf.one_hot(tf.cast(y_true_f, tf.int32), depth=tf.shape(y_pred)[-1])\n",
    "    \n",
    "    # üìä C√ÅLCULO DO DICE COEFFICIENT POR CLASSE\n",
    "    # Interse√ß√£o: onde ambos ground truth e predi√ß√£o concordam\n",
    "    intersection = tf.reduce_sum(y_true_one_hot * y_pred_f, axis=0)\n",
    "    \n",
    "    # C√°lculo do coeficiente Dice para cada classe\n",
    "    # Numerador: 2 * interse√ß√£o + smooth (smoothing para estabilidade)\n",
    "    # Denominador: soma de pixels true + soma de pixels pred + smooth\n",
    "    dice_coef = (2. * intersection + smooth) / (\n",
    "        tf.reduce_sum(y_true_one_hot, axis=0) + tf.reduce_sum(y_pred_f, axis=0) + smooth\n",
    "    )\n",
    "    \n",
    "    # üéØ RETORNAR LOSS (1 - Dice para minimiza√ß√£o)\n",
    "    return 1 - tf.reduce_mean(dice_coef)\n",
    "\n",
    "def focal_loss_modular(y_true, y_pred, alpha=None, gamma=None):\n",
    "    \"\"\"\n",
    "    üî• FOCAL LOSS - Solu√ß√£o para classes desbalanceadas\n",
    "    \n",
    "    =====================================================\n",
    "    üìä CONCEITO MATEM√ÅTICO:\n",
    "    ‚Ä¢ FL(p_t) = -Œ±_t * (1-p_t)^Œ≥ * log(p_t)\n",
    "    ‚Ä¢ Reduz peso de exemplos \"f√°ceis\", foca nos \"dif√≠ceis\"\n",
    "    ‚Ä¢ Œ± (alpha): balanceia classes positivas vs negativas\n",
    "    ‚Ä¢ Œ≥ (gamma): controla foco em exemplos dif√≠ceis\n",
    "    \n",
    "    üéØ POR QUE RESOLVE DESBALANCEAMENTO:\n",
    "    ‚Ä¢ Classe dominante (background) tem muitos exemplos f√°ceis\n",
    "    ‚Ä¢ Focal Loss reduz peso desses exemplos f√°ceis\n",
    "    ‚Ä¢ For√ßa a rede a focar em bordas e regi√µes dif√≠ceis\n",
    "    ‚Ä¢ Œ± ajusta o balan√ßo entre foreground e background\n",
    "    \n",
    "    üìà PAR√ÇMETROS T√çPICOS:\n",
    "    ‚Ä¢ Œ± = 0.8: favorece classes positivas (cats/dogs)\n",
    "    ‚Ä¢ Œ≥ = 2.0: foco moderado em exemplos dif√≠ceis\n",
    "    ‚Ä¢ Œ≥ = 0: vira CrossEntropy normal\n",
    "    ‚Ä¢ Œ≥ > 2: foco muito agressivo\n",
    "    =====================================================\n",
    "    \"\"\"\n",
    "    if alpha is None:\n",
    "        alpha = LOSS_PARAMS['focal_alpha']  # Usa valor da configura√ß√£o\n",
    "    if gamma is None:\n",
    "        gamma = LOSS_PARAMS['focal_gamma']   # Usa valor da configura√ß√£o\n",
    "    \n",
    "    # üîß PREPARA√á√ÉO DOS TENSORES\n",
    "    y_true = tf.cast(y_true, tf.int32)  # Garantir tipo inteiro\n",
    "    # Converter para one-hot encoding\n",
    "    y_true_one_hot = tf.one_hot(y_true, depth=tf.shape(y_pred)[-1])\n",
    "    y_true_one_hot = tf.reshape(y_true_one_hot, tf.shape(y_pred))\n",
    "    \n",
    "    # üõ°Ô∏è PROTE√á√ÉO CONTRA LOG(0)\n",
    "    # Clipar predi√ß√µes para evitar log(0) = -‚àû\n",
    "    epsilon = tf.keras.backend.epsilon()  # Valor muito pequeno (~1e-7)\n",
    "    y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)\n",
    "    \n",
    "    # üìä C√ÅLCULO DOS FATORES DE PESO\n",
    "    # Œ±_t: aplica Œ± para classes positivas, (1-Œ±) para negativas\n",
    "    alpha_t = alpha * y_true_one_hot + (1 - alpha) * (1 - y_true_one_hot)\n",
    "    \n",
    "    # p_t: probabilidade da classe correta\n",
    "    p_t = y_true_one_hot * y_pred + (1 - y_true_one_hot) * (1 - y_pred)\n",
    "    \n",
    "    # üéØ APLICAR F√ìRMULA DA FOCAL LOSS\n",
    "    # (1-p_t)^Œ≥: reduz peso quando p_t √© alto (exemplo f√°cil)\n",
    "    # log(p_t): componente de cross-entropy\n",
    "    focal_loss = -alpha_t * tf.pow((1 - p_t), gamma) * tf.math.log(p_t)\n",
    "    \n",
    "    return tf.reduce_mean(focal_loss)\n",
    "\n",
    "def combined_loss_modular(y_true, y_pred, weights=None):\n",
    "    \"\"\"\n",
    "    ‚öñÔ∏è COMBINED LOSS - H√≠brida otimizada para segmenta√ß√£o\n",
    "    \n",
    "    =====================================================\n",
    "    üéØ ESTRAT√âGIA DE COMBINA√á√ÉO:\n",
    "    ‚Ä¢ CrossEntropy (40%): Estabilidade base e converg√™ncia\n",
    "    ‚Ä¢ Dice Loss (40%): Foco na sobreposi√ß√£o precisa de regi√µes\n",
    "    ‚Ä¢ Focal Loss (20%): Tratamento de classes desbalanceadas\n",
    "    \n",
    "    üí° POR QUE FUNCIONA MELHOR:\n",
    "    ‚Ä¢ CCE: Fornece gradientes est√°veis e converg√™ncia confi√°vel\n",
    "    ‚Ä¢ Dice: Otimiza diretamente a m√©trica que nos importa (IoU)\n",
    "    ‚Ä¢ Focal: Resolve problema de background dominante\n",
    "    \n",
    "    üîß PESOS OTIMIZADOS:\n",
    "    ‚Ä¢ [0.4, 0.4, 0.2]: Balanceado para a maioria dos casos\n",
    "    ‚Ä¢ Para dados muito desbalanceados: [0.3, 0.4, 0.3]\n",
    "    ‚Ä¢ Para bordas precisas: [0.2, 0.6, 0.2]\n",
    "    =====================================================\n",
    "    \"\"\"\n",
    "    if weights is None:\n",
    "        weights = LOSS_PARAMS['loss_weights']  # Usa configura√ß√£o padr√£o\n",
    "    \n",
    "    # 1Ô∏è‚É£ CATEGORICAL CROSSENTROPY (Base s√≥lida)\n",
    "    # Fornece gradientes est√°veis e converg√™ncia confi√°vel\n",
    "    cce = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)\n",
    "    cce = tf.reduce_mean(cce)\n",
    "    \n",
    "    # 2Ô∏è‚É£ DICE LOSS (Sobreposi√ß√£o de regi√µes)\n",
    "    # Otimiza diretamente a qualidade da segmenta√ß√£o\n",
    "    dice = dice_loss_modular(y_true, y_pred)\n",
    "    \n",
    "    # 3Ô∏è‚É£ FOCAL LOSS (Classes desbalanceadas)\n",
    "    # Trata o problema de background dominante\n",
    "    focal = focal_loss_modular(y_true, y_pred)\n",
    "    \n",
    "    # üéØ COMBINA√á√ÉO PONDERADA FINAL\n",
    "    # Soma ponderada otimizada empiricamente\n",
    "    total_loss = weights[0] * cce + weights[1] * dice + weights[2] * focal\n",
    "    \n",
    "    return total_loss\n",
    "\n",
    "def mean_iou_modular(y_true, y_pred, num_classes=None):\n",
    "    \"\"\"\n",
    "    üìè MEAN IoU - M√©trica padr√£o para segmenta√ß√£o sem√¢ntica\n",
    "    \n",
    "    =====================================================\n",
    "    üìä CONCEITO MATEM√ÅTICO:\n",
    "    ‚Ä¢ IoU = |A ‚à© B| / |A ‚à™ B| (Intersection over Union)\n",
    "    ‚Ä¢ Calcula para cada classe individualmente\n",
    "    ‚Ä¢ Mean IoU = m√©dia de IoU de todas as classes\n",
    "    \n",
    "    üéØ POR QUE √â A M√âTRICA PRINCIPAL:\n",
    "    ‚Ä¢ Mede qualidade da segmenta√ß√£o diretamente\n",
    "    ‚Ä¢ Range [0,1] onde 1 = segmenta√ß√£o perfeita\n",
    "    ‚Ä¢ Penaliza tanto falsos positivos quanto falsos negativos\n",
    "    ‚Ä¢ Independente de desbalanceamento de classes\n",
    "    \n",
    "    üìà INTERPRETA√á√ÉO:\n",
    "    ‚Ä¢ 0.9-1.0: Excelente segmenta√ß√£o\n",
    "    ‚Ä¢ 0.7-0.9: Boa segmenta√ß√£o\n",
    "    ‚Ä¢ 0.5-0.7: Segmenta√ß√£o razo√°vel\n",
    "    ‚Ä¢ <0.5: Necessita melhorias\n",
    "    =====================================================\n",
    "    \"\"\"\n",
    "    if num_classes is None:\n",
    "        num_classes = CONFIG['NUM_CLASSES']\n",
    "    \n",
    "    # üîß PREPARA√á√ÉO DOS TENSORES\n",
    "    # Converter predi√ß√µes softmax para √≠ndices de classe\n",
    "    y_pred = tf.argmax(y_pred, axis=-1)  # [batch, height, width]\n",
    "    y_true = tf.cast(y_true, tf.int32)\n",
    "    y_pred = tf.cast(y_pred, tf.int32)\n",
    "    \n",
    "    # Achatar tensores para facilitar c√°lculo\n",
    "    y_true = tf.reshape(y_true, [-1])    # [batch*height*width]\n",
    "    y_pred = tf.reshape(y_pred, [-1])    # [batch*height*width]\n",
    "    \n",
    "    # üìä CALCULAR IoU PARA CADA CLASSE\n",
    "    ious = []\n",
    "    for class_id in range(num_classes):\n",
    "        # M√°scaras booleanas para classe atual\n",
    "        true_class = tf.equal(y_true, class_id)   # Pixels ground truth da classe\n",
    "        pred_class = tf.equal(y_pred, class_id)   # Pixels preditos da classe\n",
    "        \n",
    "        # üéØ C√ÅLCULO DE INTERSE√á√ÉO E UNI√ÉO\n",
    "        # Interse√ß√£o: pixels corretos da classe (ambos True)\n",
    "        intersection = tf.reduce_sum(tf.cast(tf.logical_and(true_class, pred_class), tf.float32))\n",
    "        \n",
    "        # Uni√£o: todos os pixels da classe (true OR pred)\n",
    "        union = tf.reduce_sum(tf.cast(tf.logical_or(true_class, pred_class), tf.float32))\n",
    "        \n",
    "        # üõ°Ô∏è PROTE√á√ÉO CONTRA DIVIS√ÉO POR ZERO\n",
    "        # Se classe n√£o aparece, considera IoU = 1.0 (perfeito)\n",
    "        iou = tf.cond(tf.equal(union, 0), \n",
    "                     lambda: 1.0,  # Classe ausente = perfeito\n",
    "                     lambda: intersection / union)  # IoU normal\n",
    "        ious.append(iou)\n",
    "    \n",
    "    # üìà RETORNAR M√âDIA DE TODAS AS CLASSES\n",
    "    return tf.reduce_mean(tf.stack(ious))\n",
    "\n",
    "def iou_per_class_modular(y_true, y_pred, num_classes=None):\n",
    "    \"\"\"\n",
    "    üîç IoU POR CLASSE - An√°lise detalhada de performance\n",
    "    \n",
    "    Retorna IoU individual para cada classe.\n",
    "    √ötil para identificar quais classes est√£o sendo mal segmentadas.\n",
    "    \"\"\"\n",
    "    if num_classes is None:\n",
    "        num_classes = CONFIG['NUM_CLASSES']\n",
    "    \n",
    "    # Mesmo processamento do mean_iou_modular\n",
    "    y_pred = tf.argmax(y_pred, axis=-1)\n",
    "    y_true = tf.cast(y_true, tf.int32)\n",
    "    y_pred = tf.cast(y_pred, tf.int32)\n",
    "    \n",
    "    y_true = tf.reshape(y_true, [-1])\n",
    "    y_pred = tf.reshape(y_pred, [-1])\n",
    "    \n",
    "    # Retornar dicion√°rio com IoU por classe\n",
    "    ious = {}\n",
    "    for class_id in range(num_classes):\n",
    "        true_class = tf.equal(y_true, class_id)\n",
    "        pred_class = tf.equal(y_pred, class_id)\n",
    "        \n",
    "        intersection = tf.reduce_sum(tf.cast(tf.logical_and(true_class, pred_class), tf.float32))\n",
    "        union = tf.reduce_sum(tf.cast(tf.logical_or(true_class, pred_class), tf.float32))\n",
    "        \n",
    "        iou = tf.cond(tf.equal(union, 0), \n",
    "                     lambda: 1.0, \n",
    "                     lambda: intersection / union)\n",
    "        ious[f'iou_class_{class_id}'] = iou  # Ex: iou_class_0, iou_class_1, iou_class_2\n",
    "    \n",
    "    return ious\n",
    "\n",
    "# üìö DICION√ÅRIOS DE FUN√á√ïES DISPON√çVEIS\n",
    "# =====================================\n",
    "# Organiza todas as fun√ß√µes para f√°cil acesso e experimenta√ß√£o\n",
    "# =====================================\n",
    "\n",
    "# üéØ LOSS FUNCTIONS DISPON√çVEIS\n",
    "LOSS_FUNCTIONS = {\n",
    "    'dice': dice_loss_modular,          # Para sobreposi√ß√£o precisa\n",
    "    'focal': focal_loss_modular,        # Para classes desbalanceadas\n",
    "    'combined': combined_loss_modular   # ü•á PRINCIPAL - H√≠brida otimizada\n",
    "}\n",
    "\n",
    "# üìè M√âTRICAS DISPON√çVEIS\n",
    "METRICS_FUNCTIONS = {\n",
    "    'mean_iou': mean_iou_modular,           # ü•á PRINCIPAL - M√©trica padr√£o\n",
    "    'iou_per_class': iou_per_class_modular  # Para an√°lise detalhada\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Sistema completo de Loss e M√©tricas implementado!\")\n",
    "print()\n",
    "print(\"üéØ RESUMO DAS FUN√á√ïES:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"üé≤ Dice Loss: Mede sobreposi√ß√£o, √≥tima para bordas precisas\")\n",
    "print(\"üî• Focal Loss: Foca em exemplos dif√≠ceis, resolve desbalanceamento\")\n",
    "print(\"‚öñÔ∏è Combined Loss: H√≠brida otimizada (CCE + Dice + Focal)\")\n",
    "print(\"üìè Mean IoU: M√©trica padr√£o ouro para segmenta√ß√£o\")\n",
    "print(\"üîç IoU per Class: An√°lise detalhada de performance por classe\")\n",
    "print()\n",
    "print(\"üèÜ CONFIGURA√á√ÉO ATUAL:\")\n",
    "print(f\"üéØ Loss principal: combined_loss_modular\")\n",
    "print(f\"‚öñÔ∏è Pesos: CCE={LOSS_PARAMS['loss_weights'][0]}, Dice={LOSS_PARAMS['loss_weights'][1]}, Focal={LOSS_PARAMS['loss_weights'][2]}\")\n",
    "print(f\"üî• Focal: Œ±={LOSS_PARAMS['focal_alpha']}, Œ≥={LOSS_PARAMS['focal_gamma']}\")\n",
    "print(f\"üé≤ Dice smoothing: {LOSS_PARAMS['dice_smooth']}\")\n",
    "print(\"üí° DICA: Use 'combined' para melhor performance geral!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "9673aa07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèóÔ∏è Implementando todas as arquiteturas U-Net...\n",
      "‚úÖ Todas as arquiteturas U-Net implementadas!\n",
      "üèóÔ∏è Dispon√≠veis: ['unet_base', 'unet_improved', 'unet_attention']\n",
      "\n",
      "üìä COMPARA√á√ÉO DAS ARQUITETURAS:\n",
      "----------------------------------------\n",
      "üèóÔ∏è unet_base:\n",
      "   ‚Ä¢ Simplicidade m√°xima, baseline de refer√™ncia\n",
      "   ‚Ä¢ Treinamento r√°pido, ideal para aprendizado\n",
      "\n",
      "üöÄ unet_improved (RECOMENDADA):\n",
      "   ‚Ä¢ BatchNormalization + Dropout + He Normal\n",
      "   ‚Ä¢ Conv2DTranspose, melhor estabilidade\n",
      "   ‚Ä¢ Boa performance/velocidade, ideal para produ√ß√£o\n",
      "\n",
      "üéØ unet_attention (AVAN√áADA):\n",
      "   ‚Ä¢ Estado da arte com Attention Gates\n",
      "   ‚Ä¢ Foco em regi√µes importantes, m√°xima precis√£o\n",
      "   ‚Ä¢ Ideal para casos complexos e objetos pequenos\n",
      "\n",
      "üí° RECOMENDA√á√ÉO:\n",
      "   ü•á Para produ√ß√£o: 'unet_improved'\n",
      "   üß™ Para experimentos: 'unet_attention'\n",
      "   üìö Para aprendizado: 'unet_base'\n"
     ]
    }
   ],
   "source": [
    "# üèóÔ∏è M√ìDULO COMPLETO: ARQUITETURAS U-NET\n",
    "print(\"üèóÔ∏è Implementando todas as arquiteturas U-Net...\")\n",
    "\n",
    "def create_unet_base(input_size=None, num_classes=None):\n",
    "    \"\"\"\n",
    "    U-Net b√°sica original (Ronneberger et al., 2015)\n",
    "    \n",
    "    Arquitetura cl√°ssica:\n",
    "    - Encoder: reduz resolu√ß√£o, aumenta canais\n",
    "    - Bottleneck: representa√ß√£o mais abstrata  \n",
    "    - Decoder: reconstr√≥i resolu√ß√£o com skip connections\n",
    "    - Skip connections: preservam detalhes finos\n",
    "    \"\"\"\n",
    "    if input_size is None:\n",
    "        input_size = (*CONFIG['IMG_SIZE'], 3)\n",
    "    if num_classes is None:\n",
    "        num_classes = CONFIG['NUM_CLASSES']\n",
    "    \n",
    "    inputs = Input(input_size)\n",
    "    \n",
    "    # ENCODER (Caminho Descendente)\n",
    "    # Block 1: 256x256 -> 128x128\n",
    "    conv1 = Conv2D(64, 3, activation='relu', padding='same')(inputs)\n",
    "    conv1 = Conv2D(64, 3, activation='relu', padding='same')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    \n",
    "    # Block 2: 128x128 -> 64x64\n",
    "    conv2 = Conv2D(128, 3, activation='relu', padding='same')(pool1)\n",
    "    conv2 = Conv2D(128, 3, activation='relu', padding='same')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    \n",
    "    # Block 3: 64x64 -> 32x32\n",
    "    conv3 = Conv2D(256, 3, activation='relu', padding='same')(pool2)\n",
    "    conv3 = Conv2D(256, 3, activation='relu', padding='same')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    \n",
    "    # Block 4: 32x32 -> 16x16\n",
    "    conv4 = Conv2D(512, 3, activation='relu', padding='same')(pool3)\n",
    "    conv4 = Conv2D(512, 3, activation='relu', padding='same')(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "    \n",
    "    # BOTTLENECK: 16x16 (menor resolu√ß√£o, maior abstra√ß√£o)\n",
    "    conv5 = Conv2D(1024, 3, activation='relu', padding='same')(pool4)\n",
    "    conv5 = Conv2D(1024, 3, activation='relu', padding='same')(conv5)\n",
    "    \n",
    "    # DECODER (Caminho Ascendente com Skip Connections)\n",
    "    # Up 1: 16x16 -> 32x32 + skip connection\n",
    "    up6 = Conv2D(512, 2, activation='relu', padding='same')(UpSampling2D(size=(2,2))(conv5))\n",
    "    merge6 = concatenate([conv4, up6], axis=3)  # Skip connection\n",
    "    conv6 = Conv2D(512, 3, activation='relu', padding='same')(merge6)\n",
    "    conv6 = Conv2D(512, 3, activation='relu', padding='same')(conv6)\n",
    "    \n",
    "    # Up 2: 32x32 -> 64x64 + skip connection\n",
    "    up7 = Conv2D(256, 2, activation='relu', padding='same')(UpSampling2D(size=(2,2))(conv6))\n",
    "    merge7 = concatenate([conv3, up7], axis=3)  # Skip connection\n",
    "    conv7 = Conv2D(256, 3, activation='relu', padding='same')(merge7)\n",
    "    conv7 = Conv2D(256, 3, activation='relu', padding='same')(conv7)\n",
    "    \n",
    "    # Up 3: 64x64 -> 128x128 + skip connection\n",
    "    up8 = Conv2D(128, 2, activation='relu', padding='same')(UpSampling2D(size=(2,2))(conv7))\n",
    "    merge8 = concatenate([conv2, up8], axis=3)  # Skip connection\n",
    "    conv8 = Conv2D(128, 3, activation='relu', padding='same')(merge8)\n",
    "    conv8 = Conv2D(128, 3, activation='relu', padding='same')(conv8)\n",
    "    \n",
    "    # Up 4: 128x128 -> 256x256 + skip connection\n",
    "    up9 = Conv2D(64, 2, activation='relu', padding='same')(UpSampling2D(size=(2,2))(conv8))\n",
    "    merge9 = concatenate([conv1, up9], axis=3)  # Skip connection\n",
    "    conv9 = Conv2D(64, 3, activation='relu', padding='same')(merge9)\n",
    "    conv9 = Conv2D(64, 3, activation='relu', padding='same')(conv9)\n",
    "    \n",
    "    # OUTPUT: Classifica√ß√£o final por pixel\n",
    "    outputs = Conv2D(num_classes, 1, activation='softmax')(conv9)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "def create_unet_improved(input_size=None, num_classes=None, dropout_rates=None):\n",
    "    \"\"\"\n",
    "    U-Net melhorada com t√©cnicas modernas\n",
    "    \n",
    "    Melhorias implementadas:\n",
    "    - BatchNormalization: estabiliza treinamento\n",
    "    - Dropout: previne overfitting\n",
    "    - He Normal: inicializa√ß√£o otimizada\n",
    "    - Conv2DTranspose: upsampling aprend√≠vel\n",
    "    - Dropout progressivo: mais dropout no bottleneck\n",
    "    \"\"\"\n",
    "    if input_size is None:\n",
    "        input_size = (*CONFIG['IMG_SIZE'], 3)\n",
    "    if num_classes is None:\n",
    "        num_classes = CONFIG['NUM_CLASSES']\n",
    "    if dropout_rates is None:\n",
    "        dropout_rates = [0.1, 0.2, 0.3, 0.4, 0.5]  # Progressivo\n",
    "    \n",
    "    inputs = Input(input_size)\n",
    "    \n",
    "    # ENCODER MELHORADO com BatchNorm e Dropout\n",
    "    # Block 1: BatchNorm + Dropout leve\n",
    "    conv1 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal')(inputs)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv1)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    pool1 = Dropout(dropout_rates[0])(pool1)\n",
    "    # Block 2: Dropout moderado\n",
    "    conv2 = Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool1)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    pool2 = Dropout(dropout_rates[1])(pool2)\n",
    "    \n",
    "    # Block 3: Dropout aumentando\n",
    "    conv3 = Conv2D(256, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool2)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Conv2D(256, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv3)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    pool3 = Dropout(dropout_rates[2])(pool3)\n",
    "    \n",
    "    # Block 4: Dropout alto\n",
    "    conv4 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool3)\n",
    "    conv4 = BatchNormalization()(conv4)\n",
    "    conv4 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv4)\n",
    "    conv4 = BatchNormalization()(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "    pool4 = Dropout(dropout_rates[3])(pool4)\n",
    "    \n",
    "    # BOTTLENECK: Dropout m√°ximo\n",
    "    conv5 = Conv2D(1024, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool4)\n",
    "    conv5 = BatchNormalization()(conv5)\n",
    "    conv5 = Conv2D(1024, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv5)\n",
    "    conv5 = BatchNormalization()(conv5)\n",
    "    conv5 = Dropout(dropout_rates[4])(conv5)\n",
    "    \n",
    "    # DECODER MELHORADO com Conv2DTranspose\n",
    "    # Up 1: Conv2DTranspose (upsampling aprend√≠vel)\n",
    "    up6 = Conv2DTranspose(512, 2, strides=(2, 2), padding='same', kernel_initializer='he_normal')(conv5)\n",
    "    up6 = BatchNormalization()(up6)\n",
    "    merge6 = concatenate([conv4, up6], axis=3)\n",
    "    conv6 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge6)\n",
    "    conv6 = BatchNormalization()(conv6)\n",
    "    conv6 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv6)\n",
    "    conv6 = BatchNormalization()(conv6)\n",
    "    conv6 = Dropout(dropout_rates[3])(conv6)  # Dropout espelhado\n",
    "    \n",
    "    # Up 2\n",
    "    up7 = Conv2DTranspose(256, 2, strides=(2, 2), padding='same', kernel_initializer='he_normal')(conv6)\n",
    "    up7 = BatchNormalization()(up7)\n",
    "    merge7 = concatenate([conv3, up7], axis=3)\n",
    "    conv7 = Conv2D(256, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge7)\n",
    "    conv7 = BatchNormalization()(conv7)\n",
    "    conv7 = Conv2D(256, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv7)\n",
    "    conv7 = BatchNormalization()(conv7)\n",
    "    conv7 = Dropout(dropout_rates[2])(conv7)\n",
    "    \n",
    "    # Up 3\n",
    "    up8 = Conv2DTranspose(128, 2, strides=(2, 2), padding='same', kernel_initializer='he_normal')(conv7)\n",
    "    up8 = BatchNormalization()(up8)\n",
    "    merge8 = concatenate([conv2, up8], axis=3)\n",
    "    conv8 = Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge8)\n",
    "    conv8 = BatchNormalization()(conv8)\n",
    "    conv8 = Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv8)\n",
    "    conv8 = BatchNormalization()(conv8)\n",
    "    conv8 = Dropout(dropout_rates[1])(conv8)\n",
    "    \n",
    "    # Up 4\n",
    "    up9 = Conv2DTranspose(64, 2, strides=(2, 2), padding='same', kernel_initializer='he_normal')(conv8)\n",
    "    up9 = BatchNormalization()(up9)\n",
    "    merge9 = concatenate([conv1, up9], axis=3)\n",
    "    conv9 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge9)\n",
    "    conv9 = BatchNormalization()(conv9)\n",
    "    conv9 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv9)\n",
    "    conv9 = BatchNormalization()(conv9)\n",
    "    conv9 = Dropout(dropout_rates[0])(conv9)\n",
    "    \n",
    "    # OUTPUT MELHORADO\n",
    "    outputs = Conv2D(num_classes, 1, activation='softmax', name='output')(conv9)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "def create_unet_attention(input_size=None, num_classes=None):\n",
    "    \"\"\"\n",
    "    U-Net com Attention Gates (estado da arte)\n",
    "    \n",
    "    Attention Gates inova√ß√£o:\n",
    "    - Focam a rede nas regi√µes relevantes\n",
    "    - Suprimem informa√ß√µes irrelevantes\n",
    "    - Melhoram precis√£o sem aumentar par√¢metros\n",
    "    - Especialmente bons para objetos pequenos\n",
    "    \"\"\"\n",
    "    if input_size is None:\n",
    "        input_size = (*CONFIG['IMG_SIZE'], 3)\n",
    "    if num_classes is None:\n",
    "        num_classes = CONFIG['NUM_CLASSES']\n",
    "    \n",
    "    def attention_gate(F_g, F_l, F_int):\n",
    "        \"\"\"Attention Gate implementation\"\"\"\n",
    "        # Transformar gating signal\n",
    "        W_g = Conv2D(F_int, 1, padding='same')(F_g)\n",
    "        W_g = BatchNormalization()(W_g)\n",
    "        \n",
    "        # Transformar input features\n",
    "        W_x = Conv2D(F_int, 1, padding='same')(F_l)\n",
    "        W_x = BatchNormalization()(W_x)\n",
    "        \n",
    "        # Combinar e aplicar ativa√ß√£o\n",
    "        psi = Activation('relu')(Add()([W_g, W_x]))\n",
    "        psi = Conv2D(1, 1, padding='same')(psi)\n",
    "        psi = BatchNormalization()(psi)\n",
    "        psi = Activation('sigmoid')(psi)  # Attention weights [0,1]\n",
    "        \n",
    "        # Aplicar attention (multiplica√ß√£o elemento-wise)\n",
    "        return Multiply()([F_l, psi])\n",
    "    \n",
    "    inputs = Input(input_size)\n",
    "    \n",
    "    # ENCODER (igual ao melhorado, mas sem dropout para simplicidade)\n",
    "    conv1 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal')(inputs)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv1)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    \n",
    "    conv2 = Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool1)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    \n",
    "    conv3 = Conv2D(256, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool2)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Conv2D(256, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv3)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    \n",
    "    conv4 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool3)\n",
    "    conv4 = BatchNormalization()(conv4)\n",
    "    conv4 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv4)\n",
    "    conv4 = BatchNormalization()(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "    \n",
    "    # BOTTLENECK\n",
    "    conv5 = Conv2D(1024, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool4)\n",
    "    conv5 = BatchNormalization()(conv5)\n",
    "    conv5 = Conv2D(1024, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv5)\n",
    "    conv5 = BatchNormalization()(conv5)\n",
    "    \n",
    "    # DECODER COM ATTENTION GATES\n",
    "    # Up 1: Attention + Skip Connection\n",
    "    up6 = Conv2DTranspose(512, 2, strides=(2, 2), padding='same')(conv5)\n",
    "    up6 = BatchNormalization()(up6)\n",
    "    att6 = attention_gate(up6, conv4, 256)  # Aplicar attention\n",
    "    merge6 = concatenate([att6, up6], axis=3)  # Skip connection com attention\n",
    "    conv6 = Conv2D(512, 3, activation='relu', padding='same')(merge6)\n",
    "    conv6 = BatchNormalization()(conv6)\n",
    "    \n",
    "    # Up 2: Attention + Skip Connection  \n",
    "    up7 = Conv2DTranspose(256, 2, strides=(2, 2), padding='same')(conv6)\n",
    "    up7 = BatchNormalization()(up7)\n",
    "    att7 = attention_gate(up7, conv3, 128)  # Aplicar attention\n",
    "    merge7 = concatenate([att7, up7], axis=3)\n",
    "    conv7 = Conv2D(256, 3, activation='relu', padding='same')(merge7)\n",
    "    conv7 = BatchNormalization()(conv7)\n",
    "    \n",
    "    # Up 3: Attention + Skip Connection\n",
    "    up8 = Conv2DTranspose(128, 2, strides=(2, 2), padding='same')(conv7)\n",
    "    up8 = BatchNormalization()(up8)\n",
    "    att8 = attention_gate(up8, conv2, 64)  # Aplicar attention\n",
    "    merge8 = concatenate([att8, up8], axis=3)\n",
    "    conv8 = Conv2D(128, 3, activation='relu', padding='same')(merge8)\n",
    "    conv8 = BatchNormalization()(conv8)\n",
    "    \n",
    "    # Up 4: Attention + Skip Connection\n",
    "    up9 = Conv2DTranspose(64, 2, strides=(2, 2), padding='same')(conv8)\n",
    "    up9 = BatchNormalization()(up9)\n",
    "    att9 = attention_gate(up9, conv1, 32)  # Aplicar attention\n",
    "    merge9 = concatenate([att9, up9], axis=3)\n",
    "    conv9 = Conv2D(64, 3, activation='relu', padding='same')(merge9)\n",
    "    conv9 = BatchNormalization()(conv9)\n",
    "    \n",
    "    # OUTPUT\n",
    "    outputs = Conv2D(num_classes, 1, activation='softmax')(conv9)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "# Dicion√°rio de arquiteturas dispon√≠veis\n",
    "ARCHITECTURES = {\n",
    "    'unet_base': create_unet_base,\n",
    "    'unet_improved': create_unet_improved,\n",
    "    'unet_attention': create_unet_attention\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Todas as arquiteturas U-Net implementadas!\")\n",
    "print(f\"üèóÔ∏è Dispon√≠veis: {list(ARCHITECTURES.keys())}\")\n",
    "print()\n",
    "print(\"üìä COMPARA√á√ÉO DAS ARQUITETURAS:\")\n",
    "print(\"-\" * 40)\n",
    "print(\"üèóÔ∏è unet_base:\")\n",
    "print(\"   ‚Ä¢ Simplicidade m√°xima, baseline de refer√™ncia\")\n",
    "print(\"   ‚Ä¢ Treinamento r√°pido, ideal para aprendizado\")\n",
    "print()\n",
    "print(\"üöÄ unet_improved (RECOMENDADA):\")\n",
    "print(\"   ‚Ä¢ BatchNormalization + Dropout + He Normal\")\n",
    "print(\"   ‚Ä¢ Conv2DTranspose, melhor estabilidade\")\n",
    "print(\"   ‚Ä¢ Boa performance/velocidade, ideal para produ√ß√£o\")\n",
    "print()\n",
    "print(\"üéØ unet_attention (AVAN√áADA):\")\n",
    "print(\"   ‚Ä¢ Estado da arte com Attention Gates\")\n",
    "print(\"   ‚Ä¢ Foco em regi√µes importantes, m√°xima precis√£o\")\n",
    "print(\"   ‚Ä¢ Ideal para casos complexos e objetos pequenos\")\n",
    "print()\n",
    "print(\"üí° RECOMENDA√á√ÉO:\")\n",
    "print(\"   ü•á Para produ√ß√£o: 'unet_improved'\")\n",
    "print(\"   üß™ Para experimentos: 'unet_attention'\")\n",
    "print(\"   üìö Para aprendizado: 'unet_base'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "1405b5f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé® Carregando data augmentation completo...\n",
      "‚úÖ Data augmentation completo carregado!\n",
      "üé® Tipos dispon√≠veis: ['none', 'basic', 'advanced']\n",
      "üîß Par√¢metros: rotation=25¬∞, \n",
      "   shift=0.15, zoom=0.15\n",
      "üîó Fun√ß√£o principal: create_paired_generators()\n",
      "üí° Generators sempre sincronizados (mesmo seed)\n"
     ]
    }
   ],
   "source": [
    "# üîÑ M√ìDULO 4: DATA AUGMENTATION COMPLETO\n",
    "print(\"üé® Carregando data augmentation completo...\")\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "def create_basic_augmentation():\n",
    "    \"\"\"\n",
    "    Data augmentation b√°sico e seguro\n",
    "    \n",
    "    Transforma√ß√µes conservadoras:\n",
    "    - Rota√ß√£o limitada (15¬∞)\n",
    "    - Shift limitado (10%)\n",
    "    - Apenas flip horizontal\n",
    "    - Normaliza√ß√£o autom√°tica\n",
    "    \n",
    "    Ideal para: primeiros testes, dados limitados\n",
    "    \"\"\"\n",
    "    return ImageDataGenerator(\n",
    "        rotation_range=15,          # Rota√ß√£o at√© 15¬∞\n",
    "        width_shift_range=0.1,      # Shift horizontal 10%\n",
    "        height_shift_range=0.1,     # Shift vertical 10%  \n",
    "        horizontal_flip=True,       # Flip horizontal\n",
    "        rescale=1./255             # Normaliza√ß√£o [0,1]\n",
    "    )\n",
    "\n",
    "def create_mask_augmentation_basic():\n",
    "    \"\"\"\n",
    "    Data augmentation b√°sico para m√°scaras\n",
    "    \n",
    "    Importante: m√°scaras N√ÉO podem ter:\n",
    "    - Rescale (valores devem ser inteiros 0,1,2)\n",
    "    - Brightness/contrast (altera classes)\n",
    "    - Channel shift (m√°scaras s√£o single-channel)\n",
    "    \"\"\"\n",
    "    return ImageDataGenerator(\n",
    "        rotation_range=15,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        horizontal_flip=True\n",
    "        # SEM rescale nem brightness!\n",
    "    )\n",
    "\n",
    "def create_advanced_augmentation(params=None):\n",
    "    \"\"\"\n",
    "    Data augmentation avan√ßado e agressivo\n",
    "    \n",
    "    Transforma√ß√µes intensas para m√°xima generaliza√ß√£o:\n",
    "    - Rota√ß√£o 25¬∞ (detecta objetos em qualquer √¢ngulo)\n",
    "    - Shifts 15% (posi√ß√µes variadas)\n",
    "    - Zoom ¬±15% (tamanhos diferentes)\n",
    "    - Shear 15¬∞ (perspectivas)\n",
    "    - Flips vertical e horizontal\n",
    "    - Brightness ¬±20% (ilumina√ß√£o)\n",
    "    - Channel shift (cores)\n",
    "    \n",
    "    Ideal para: produ√ß√£o, datasets pequenos\n",
    "    \"\"\"\n",
    "    if params is None:\n",
    "        params = AUGMENTATION_PARAMS\n",
    "    \n",
    "    return ImageDataGenerator(\n",
    "        rotation_range=params['rotation_range'],        # 25¬∞\n",
    "        width_shift_range=params['width_shift_range'],  # 15%\n",
    "        height_shift_range=params['height_shift_range'], # 15%\n",
    "        shear_range=params['shear_range'],              # 15¬∞\n",
    "        zoom_range=params['zoom_range'],                # ¬±15%\n",
    "        horizontal_flip=params['horizontal_flip'],       # True\n",
    "        vertical_flip=params['vertical_flip'],          # True  \n",
    "        brightness_range=params['brightness_range'],    # [0.8, 1.2]\n",
    "        channel_shift_range=params['channel_shift_range'], # 20\n",
    "        fill_mode=params['fill_mode'],                  # 'reflect'\n",
    "        rescale=1./255\n",
    "    )\n",
    "\n",
    "def create_mask_augmentation_advanced(params=None):\n",
    "    \"\"\"\n",
    "    Data augmentation avan√ßado para m√°scaras\n",
    "    \n",
    "    MESMAS transforma√ß√µes geom√©tricas que imagens,\n",
    "    mas SEM altera√ß√µes de cor/brilho\n",
    "    \"\"\"\n",
    "    if params is None:\n",
    "        params = AUGMENTATION_PARAMS\n",
    "    \n",
    "    return ImageDataGenerator(\n",
    "        rotation_range=params['rotation_range'],\n",
    "        width_shift_range=params['width_shift_range'],\n",
    "        height_shift_range=params['height_shift_range'],\n",
    "        shear_range=params['shear_range'],\n",
    "        zoom_range=params['zoom_range'],\n",
    "        horizontal_flip=params['horizontal_flip'],\n",
    "        vertical_flip=params['vertical_flip'],\n",
    "        fill_mode=params['fill_mode']\n",
    "        # SEM: rescale, brightness, channel_shift\n",
    "    )\n",
    "\n",
    "def apply_augmentation_to_data(X_data, Y_data, augmentation_type='advanced'):\n",
    "    \"\"\"Aplicar augmentation aos dados com valida√ß√£o\"\"\"\n",
    "    print(f\"üîÑ Aplicando augmentation: {augmentation_type}\")\n",
    "    \n",
    "    if augmentation_type == 'basic':\n",
    "        img_gen = create_basic_augmentation()\n",
    "        mask_gen = create_mask_augmentation_basic()\n",
    "    elif augmentation_type == 'advanced':\n",
    "        img_gen = create_advanced_augmentation()\n",
    "        mask_gen = create_mask_augmentation_advanced()\n",
    "    else:\n",
    "        raise ValueError(\"Tipo deve ser 'basic' ou 'advanced'\")\n",
    "    \n",
    "    # Converter para numpy se necess√°rio\n",
    "    if isinstance(X_data, list):\n",
    "        X_data = np.array(X_data)\n",
    "        print(\"üîÑ Convertido X_data para numpy array\")\n",
    "    if isinstance(Y_data, list):\n",
    "        Y_data = np.array(Y_data)\n",
    "        print(\"üîÑ Convertido Y_data para numpy array\")\n",
    "    \n",
    "    # Normalizar dados de imagem se necess√°rio\n",
    "    if X_data.max() > 1.5:\n",
    "        print(\"üîÑ Normalizando imagens [0,1]\")\n",
    "        X_data = X_data / 255.0\n",
    "    else:\n",
    "        print(\"‚úÖ Imagens j√° normalizadas\")\n",
    "    \n",
    "    print(f\"‚úÖ Dados preparados: X{X_data.shape}, Y{Y_data.shape}\")\n",
    "    return img_gen, mask_gen, X_data, Y_data\n",
    "\n",
    "def create_paired_generators(X_data, Y_data, batch_size=None, augmentation_type='advanced', seed=42):\n",
    "    \"\"\"\n",
    "    Criar geradores pareados para treinamento\n",
    "    \n",
    "    Cr√≠tico: imagem e m√°scara devem ter MESMA transforma√ß√£o!\n",
    "    - Mesmo seed garante sincroniza√ß√£o\n",
    "    - Mesmo batch_size\n",
    "    - Mesmo shuffle\n",
    "    \"\"\"\n",
    "    if batch_size is None:\n",
    "        batch_size = CONFIG['BATCH_SIZE']\n",
    "    \n",
    "    print(f\"üîó Criando generators pareados (seed={seed})\")\n",
    "    \n",
    "    img_gen, mask_gen, X_norm, Y_norm = apply_augmentation_to_data(X_data, Y_data, augmentation_type)\n",
    "    \n",
    "    # Expandir dimens√µes das m√°scaras se necess√°rio\n",
    "    if len(Y_norm.shape) == 3:\n",
    "        Y_norm = np.expand_dims(Y_norm, axis=-1)\n",
    "        print(\"üîÑ Expandindo dimens√µes das m√°scaras\")\n",
    "    \n",
    "    # Criar generators sincronizados\n",
    "    img_generator = img_gen.flow(X_norm, batch_size=batch_size, seed=seed)\n",
    "    mask_generator = mask_gen.flow(Y_norm, batch_size=batch_size, seed=seed)\n",
    "    \n",
    "    print(f\"‚úÖ Generators criados: batch_size={batch_size}\")\n",
    "    return zip(img_generator, mask_generator)\n",
    "\n",
    "# Dicion√°rio de tipos de augmentation dispon√≠veis\n",
    "AUGMENTATION_TYPES = {\n",
    "    'none': lambda: (None, None),\n",
    "    'basic': lambda: (create_basic_augmentation(), create_mask_augmentation_basic()),\n",
    "    'advanced': lambda: (create_advanced_augmentation(), create_mask_augmentation_advanced())\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Data augmentation completo carregado!\")\n",
    "print(f\"üé® Tipos dispon√≠veis: {list(AUGMENTATION_TYPES.keys())}\")\n",
    "print(f\"üîß Par√¢metros: rotation={AUGMENTATION_PARAMS['rotation_range']}¬∞, \")\n",
    "print(f\"   shift={AUGMENTATION_PARAMS['width_shift_range']}, zoom={AUGMENTATION_PARAMS['zoom_range']}\")\n",
    "print(\"üîó Fun√ß√£o principal: create_paired_generators()\")\n",
    "print(\"üí° Generators sempre sincronizados (mesmo seed)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "01402837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéõÔ∏è Carregando callbacks, schedulers e otimiza√ß√£o...\n",
      "‚úÖ Callbacks, schedulers e otimiza√ß√£o carregados!\n",
      "‚öôÔ∏è Configs dispon√≠veis: ['basic', 'advanced', 'experimental']\n",
      "üéØ Monitor padr√£o: val_mean_iou\n",
      "‚ö° Otimizadores: adam (padr√£o), adamw, sgd, rmsprop\n",
      "üìà Schedulers: cosine (recomendado), exponential, polynomial\n",
      "‚è∞ Paci√™ncia: EarlyStopping=10, ReduceLR=4\n"
     ]
    }
   ],
   "source": [
    "# ‚öôÔ∏è M√ìDULO 5: CALLBACKS, SCHEDULERS E OTIMIZA√á√ÉO COMPLETO\n",
    "print(\"üéõÔ∏è Carregando callbacks, schedulers e otimiza√ß√£o...\")\n",
    "\n",
    "from tensorflow.keras.callbacks import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "\n",
    "def create_basic_callbacks(params=None):\n",
    "    \"\"\"\n",
    "    Callbacks essenciais para qualquer treinamento\n",
    "    \n",
    "    ModelCheckpoint:\n",
    "    - Salva melhor modelo automaticamente\n",
    "    - Monitora val_mean_iou (m√©trica principal)\n",
    "    - S√≥ salva quando melhora\n",
    "    \n",
    "    EarlyStopping:\n",
    "    - Para treinamento se n√£o melhorar\n",
    "    - Evita overfitting\n",
    "    - Restaura melhores pesos\n",
    "    \"\"\"\n",
    "    if params is None:\n",
    "        params = CALLBACK_PARAMS\n",
    "    \n",
    "    callbacks = [\n",
    "        ModelCheckpoint(\n",
    "            params['model_save_path'],\n",
    "            monitor=params['monitor_metric'],    # 'val_mean_iou'\n",
    "            save_best_only=True,                # S√≥ salva se melhorar\n",
    "            mode='max',                         # Maximizar mIoU\n",
    "            verbose=1,                          # Mostrar quando salva\n",
    "            save_format='h5'\n",
    "        ),\n",
    "        EarlyStopping(\n",
    "            monitor=params['monitor_metric'],    # 'val_mean_iou'\n",
    "            patience=params['early_stopping_patience'],  # 10 √©pocas\n",
    "            restore_best_weights=True,          # Volta pro melhor\n",
    "            mode='max',                         # Maximizar mIoU\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    print(f\"‚úÖ Callbacks b√°sicos configurados!\")\n",
    "    print(f\"üìä Monitor: {params['monitor_metric']}\")\n",
    "    print(f\"üíæ Arquivo: {params['model_save_path']}\")\n",
    "    print(f\"‚è∞ Paci√™ncia: {params['early_stopping_patience']} √©pocas\")\n",
    "    \n",
    "    return callbacks\n",
    "\n",
    "def create_advanced_callbacks(params=None, log_file=None):\n",
    "    \"\"\"\n",
    "    Callbacks avan√ßados para treinamento profissional\n",
    "    \n",
    "    Adiciona aos b√°sicos:\n",
    "    - ReduceLROnPlateau: ajusta learning rate\n",
    "    - CSVLogger: salva hist√≥rico completo\n",
    "    - Configura√ß√µes otimizadas\n",
    "    \"\"\"\n",
    "    if params is None:\n",
    "        params = CALLBACK_PARAMS\n",
    "    if log_file is None:\n",
    "        log_file = 'training_log_modular.csv'\n",
    "    \n",
    "    callbacks = [\n",
    "        ModelCheckpoint(\n",
    "            params['model_save_path'],\n",
    "            monitor=params['monitor_metric'],\n",
    "            save_best_only=True,\n",
    "            mode='max',\n",
    "            verbose=1,\n",
    "            save_weights_only=False\n",
    "        ),\n",
    "        EarlyStopping(\n",
    "            monitor=params['monitor_metric'],\n",
    "            patience=params['early_stopping_patience'],\n",
    "            restore_best_weights=True,\n",
    "            mode='max',\n",
    "            verbose=1\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor=params['monitor_metric'],\n",
    "            factor=params['lr_factor'],\n",
    "            patience=params['lr_patience'],\n",
    "            min_lr=params['lr_min'],\n",
    "            mode='max',\n",
    "            verbose=1,\n",
    "            cooldown=2\n",
    "        ),\n",
    "        CSVLogger(\n",
    "            log_file,\n",
    "            append=True\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    print(f\"‚úÖ Callbacks avan√ßados configurados!\")\n",
    "    print(f\"üìâ ReduceLR: fator={params['lr_factor']}, paci√™ncia={params['lr_patience']}\")\n",
    "    print(f\"üìä CSV Log: {log_file}\")\n",
    "    \n",
    "    return callbacks\n",
    "\n",
    "def create_lr_scheduler(schedule_type='cosine', initial_lr=None, decay_steps=None):\n",
    "    \"\"\"\n",
    "    Diferentes estrat√©gias de Learning Rate Scheduling\n",
    "    \n",
    "    Cosine Decay (RECOMENDADO):\n",
    "    - Decaimento suave em formato cosseno\n",
    "    - Permite \"restarts\" naturais\n",
    "    - Boa converg√™ncia\n",
    "    \n",
    "    Exponential Decay:\n",
    "    - Decaimento exponencial cl√°ssico\n",
    "    - Previs√≠vel e est√°vel\n",
    "    \n",
    "    Polynomial Decay:\n",
    "    - Decaimento polinomial controlado\n",
    "    - Flex√≠vel com power ajust√°vel\n",
    "    \"\"\"\n",
    "    if initial_lr is None:\n",
    "        initial_lr = CONFIG['LEARNING_RATE']\n",
    "    if decay_steps is None:\n",
    "        decay_steps = CONFIG['EPOCHS'] * 100  # Estimativa\n",
    "    \n",
    "    print(f\"üìà Criando scheduler: {schedule_type}\")\n",
    "    print(f\"   Initial LR: {initial_lr}\")\n",
    "    print(f\"   Decay steps: {decay_steps}\")\n",
    "    \n",
    "    if schedule_type == 'cosine':\n",
    "        scheduler = tf.keras.optimizers.schedules.CosineDecay(\n",
    "            initial_learning_rate=initial_lr,\n",
    "            decay_steps=decay_steps,\n",
    "            alpha=0.1                           # LR final = 10% do inicial\n",
    "        )\n",
    "        print(\"   üåä Cosine: decaimento suave, converg√™ncia natural\")\n",
    "        \n",
    "    elif schedule_type == 'exponential':\n",
    "        scheduler = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "            initial_learning_rate=initial_lr,\n",
    "            decay_steps=decay_steps // 4,       # Decai a cada 1/4 do treino\n",
    "            decay_rate=0.96,                    # Reduz 4% por step\n",
    "            staircase=True                      # Passos discretos\n",
    "        )\n",
    "        print(\"   üìâ Exponential: redu√ß√£o 4% por step, previs√≠vel\")\n",
    "        \n",
    "    elif schedule_type == 'polynomial':\n",
    "        scheduler = tf.keras.optimizers.schedules.PolynomialDecay(\n",
    "            initial_learning_rate=initial_lr,\n",
    "            decay_steps=decay_steps,\n",
    "            end_learning_rate=initial_lr * 0.01, # 1% do inicial\n",
    "            power=0.9                           # Suavidade do decaimento\n",
    "        )\n",
    "        print(\"   üìê Polynomial: decaimento power=0.9, flex√≠vel\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è  Tipo desconhecido, usando LR constante\")\n",
    "        return initial_lr\n",
    "    \n",
    "    return scheduler\n",
    "\n",
    "def create_optimizer(optimizer_type='adam', learning_rate=None, **kwargs):\n",
    "    \"\"\"\n",
    "    Diferentes otimizadores para diferentes necessidades\n",
    "    \n",
    "    Adam (PADR√ÉO):\n",
    "    - Adaptativo, funciona bem out-of-the-box\n",
    "    - Boa converg√™ncia, est√°vel\n",
    "    \n",
    "    AdamW:\n",
    "    - Adam com weight decay separado\n",
    "    - Melhor regulariza√ß√£o\n",
    "    \n",
    "    SGD:\n",
    "    - Cl√°ssico, com momentum\n",
    "    - Mais lento mas √†s vezes melhor converg√™ncia final\n",
    "    \n",
    "    RMSprop:\n",
    "    - Adaptativo, boa para RNNs\n",
    "    - Alternativa ao Adam\n",
    "    \"\"\"\n",
    "    if learning_rate is None:\n",
    "        learning_rate = CONFIG['LEARNING_RATE']\n",
    "    \n",
    "    print(f\"‚ö° Criando otimizador: {optimizer_type}\")\n",
    "    print(f\"   Learning Rate: {learning_rate}\")\n",
    "    \n",
    "    if optimizer_type == 'adam':\n",
    "        optimizer = Adam(\n",
    "            learning_rate=learning_rate,\n",
    "            beta_1=0.9,         # Momentum para gradiente\n",
    "            beta_2=0.999,       # Momentum para gradiente^2\n",
    "            epsilon=1e-7,       # Estabilidade num√©rica\n",
    "            **kwargs\n",
    "        )\n",
    "        print(\"   üéØ Adam: adaptativo, est√°vel, recomendado\")\n",
    "        \n",
    "    elif optimizer_type == 'adamw':\n",
    "        optimizer = tf.keras.optimizers.AdamW(\n",
    "            learning_rate=learning_rate,\n",
    "            weight_decay=0.01,  # L2 regularization\n",
    "            beta_1=0.9,\n",
    "            beta_2=0.999,\n",
    "            epsilon=1e-7,\n",
    "            **kwargs\n",
    "        )\n",
    "        print(\"   üéØ AdamW: Adam + weight decay, melhor regulariza√ß√£o\")\n",
    "        \n",
    "    elif optimizer_type == 'sgd':\n",
    "        optimizer = tf.keras.optimizers.SGD(\n",
    "            learning_rate=learning_rate,\n",
    "            momentum=0.9,       # Momentum padr√£o\n",
    "            nesterov=True,      # Nesterov momentum\n",
    "            **kwargs\n",
    "        )\n",
    "        print(\"   üéØ SGD: cl√°ssico + momentum + nesterov\")\n",
    "        \n",
    "    elif optimizer_type == 'rmsprop':\n",
    "        optimizer = tf.keras.optimizers.RMSprop(\n",
    "            learning_rate=learning_rate,\n",
    "            rho=0.9,           # Decay rate\n",
    "            epsilon=1e-7,\n",
    "            **kwargs\n",
    "        )\n",
    "        print(\"   üéØ RMSprop: adaptativo, alternativa ao Adam\")\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(f\"Optimizer '{optimizer_type}' n√£o suportado\")\n",
    "    \n",
    "    return optimizer\n",
    "\n",
    "def create_custom_callbacks(tensorboard_log=None, reduce_lr_monitor='val_loss'):\n",
    "    \"\"\"Callbacks customizados com TensorBoard\"\"\"\n",
    "    callbacks = create_advanced_callbacks()\n",
    "    \n",
    "    if tensorboard_log:\n",
    "        callbacks.append(\n",
    "            TensorBoard(\n",
    "                log_dir=tensorboard_log,\n",
    "                histogram_freq=1,\n",
    "                write_graph=True,\n",
    "                write_images=True,\n",
    "                update_freq='epoch'\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    # Callback customizado para m√©tricas detalhadas\n",
    "    class DetailedMetrics(Callback):\n",
    "        def on_epoch_end(self, epoch, logs=None):\n",
    "            if logs:\n",
    "                print(f\"\\\\nüìä √âpoca {epoch + 1}:\")\n",
    "                print(f\"   üéØ Accuracy: {logs.get('accuracy', 0):.4f} ‚Üí Val: {logs.get('val_accuracy', 0):.4f}\")\n",
    "                print(f\"   üé™ mIoU: {logs.get('mean_iou', 0):.4f} ‚Üí Val: {logs.get('val_mean_iou', 0):.4f}\")\n",
    "                print(f\"   üìâ Loss: {logs.get('loss', 0):.4f} ‚Üí Val: {logs.get('val_loss', 0):.4f}\")\n",
    "                print(f\"   üìà LR: {logs.get('learning_rate', 0):.2e}\")\n",
    "    \n",
    "    callbacks.append(DetailedMetrics())\n",
    "    \n",
    "    return callbacks\n",
    "\n",
    "# Configura√ß√µes de otimiza√ß√£o pr√©-definidas\n",
    "OPTIMIZATION_CONFIGS = {\n",
    "    'basic': {\n",
    "        'optimizer': 'adam',\n",
    "        'callbacks': 'basic',\n",
    "        'lr_schedule': None,\n",
    "        'description': 'Configura√ß√£o simples e est√°vel'\n",
    "    },\n",
    "    'advanced': {\n",
    "        'optimizer': 'adam',\n",
    "        'callbacks': 'advanced', \n",
    "        'lr_schedule': 'cosine',\n",
    "        'description': 'Configura√ß√£o recomendada para produ√ß√£o'\n",
    "    },\n",
    "    'experimental': {\n",
    "        'optimizer': 'adamw',\n",
    "        'callbacks': 'custom',\n",
    "        'lr_schedule': 'polynomial',\n",
    "        'description': 'Configura√ß√£o experimental avan√ßada'\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Callbacks, schedulers e otimiza√ß√£o carregados!\")\n",
    "print(f\"‚öôÔ∏è Configs dispon√≠veis: {list(OPTIMIZATION_CONFIGS.keys())}\")\n",
    "print(f\"üéØ Monitor padr√£o: {CALLBACK_PARAMS['monitor_metric']}\")\n",
    "print(f\"‚ö° Otimizadores: adam (padr√£o), adamw, sgd, rmsprop\")\n",
    "print(f\"üìà Schedulers: cosine (recomendado), exponential, polynomial\")\n",
    "print(f\"‚è∞ Paci√™ncia: EarlyStopping={CALLBACK_PARAMS['early_stopping_patience']}, ReduceLR={CALLBACK_PARAMS['lr_patience']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "088c37ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî• Carregando sistema de treinamento...\n",
      "‚úÖ Sistema de treinamento carregado!\n",
      "üöÄ Fun√ß√µes dispon√≠veis:\n",
      "   ‚Ä¢ train_model_modular() - Treinamento completo\n",
      "   ‚Ä¢ quick_train_demo() - Demo r√°pida\n",
      "   ‚Ä¢ compare_architectures() - Compara√ß√£o autom√°tica\n"
     ]
    }
   ],
   "source": [
    "# üöÄ M√ìDULO 6: TREINAMENTO MODULAR\n",
    "print(\"üî• Carregando sistema de treinamento...\")\n",
    "\n",
    "def create_model_modular(architecture='unet_improved', compile_model=True, **kwargs):\n",
    "    \"\"\"Criar modelo com arquitetura especificada\"\"\"\n",
    "    if architecture not in ARCHITECTURES:\n",
    "        raise ValueError(f\"Arquitetura '{architecture}' n√£o encontrada. Dispon√≠veis: {list(ARCHITECTURES.keys())}\")\n",
    "    \n",
    "    print(f\"üèóÔ∏è Criando modelo: {architecture}\")\n",
    "    model = ARCHITECTURES[architecture](**kwargs)\n",
    "    \n",
    "    if compile_model:\n",
    "        print(\"‚öôÔ∏è Compilando modelo...\")\n",
    "        optimizer = create_optimizer()\n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss=combined_loss_modular,\n",
    "            metrics=['accuracy', mean_iou_modular]\n",
    "        )\n",
    "        print(\"‚úÖ Modelo compilado!\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def prepare_data_for_training(X_data, Y_data, validation_data=None, normalize=True):\n",
    "    \"\"\"Preparar dados para treinamento\"\"\"\n",
    "    print(\"üìä Preparando dados...\")\n",
    "    \n",
    "    # Converter para numpy arrays se necess√°rio\n",
    "    if isinstance(X_data, list):\n",
    "        X_data = np.array(X_data)\n",
    "    if isinstance(Y_data, list):\n",
    "        Y_data = np.array(Y_data)\n",
    "    \n",
    "    # Normalizar se necess√°rio\n",
    "    if normalize and X_data.max() > 1.5:\n",
    "        X_data = X_data / 255.0\n",
    "        print(\"üîÑ Dados normalizados (0-1)\")\n",
    "    \n",
    "    # Preparar dados de valida√ß√£o se fornecidos\n",
    "    if validation_data:\n",
    "        X_val, Y_val = validation_data\n",
    "        if isinstance(X_val, list):\n",
    "            X_val = np.array(X_val)\n",
    "        if isinstance(Y_val, list):\n",
    "            Y_val = np.array(Y_val)\n",
    "        \n",
    "        if normalize and X_val.max() > 1.5:\n",
    "            X_val = X_val / 255.0\n",
    "        \n",
    "        validation_data = (X_val, Y_val)\n",
    "    \n",
    "    print(f\"‚úÖ Dados preparados!\")\n",
    "    print(f\"   üìä X_train: {X_data.shape}\")\n",
    "    print(f\"   üìä Y_train: {Y_data.shape}\")\n",
    "    if validation_data:\n",
    "        print(f\"   üìä X_val: {validation_data[0].shape}\")\n",
    "        print(f\"   üìä Y_val: {validation_data[1].shape}\")\n",
    "    \n",
    "    return X_data, Y_data, validation_data\n",
    "\n",
    "def train_model_modular(\n",
    "    X_train, Y_train, \n",
    "    validation_data=None,\n",
    "    architecture='unet_improved',\n",
    "    optimization_config='advanced',\n",
    "    epochs=None,\n",
    "    batch_size=None,\n",
    "    augmentation_type='advanced',\n",
    "    verbose=1\n",
    "):\n",
    "    \"\"\"Fun√ß√£o principal de treinamento modular\"\"\"\n",
    "    \n",
    "    if epochs is None:\n",
    "        epochs = CONFIG['EPOCHS']\n",
    "    if batch_size is None:\n",
    "        batch_size = CONFIG['BATCH_SIZE']\n",
    "    \n",
    "    print(\"üöÄ INICIANDO TREINAMENTO MODULAR\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # 1. Criar modelo\n",
    "    model = create_model_modular(architecture=architecture)\n",
    "    \n",
    "    # 2. Preparar dados\n",
    "    X_train, Y_train, validation_data = prepare_data_for_training(\n",
    "        X_train, Y_train, validation_data\n",
    "    )\n",
    "    \n",
    "    # 3. Configurar callbacks\n",
    "    if optimization_config == 'basic':\n",
    "        callbacks = create_basic_callbacks()\n",
    "    elif optimization_config == 'advanced':\n",
    "        callbacks = create_advanced_callbacks()\n",
    "    elif optimization_config == 'custom':\n",
    "        callbacks = create_custom_callbacks()\n",
    "    else:\n",
    "        callbacks = create_advanced_callbacks()\n",
    "    \n",
    "    # 4. Configurar augmentation (se especificado)\n",
    "    if augmentation_type != 'none':\n",
    "        print(f\"üé® Aplicando {augmentation_type} augmentation...\")\n",
    "        # Para simplicidade, vamos treinar sem generators por enquanto\n",
    "        # Em produ√ß√£o, implementaria generators aqui\n",
    "    \n",
    "    print(f\"üìà Configura√ß√µes de treinamento:\")\n",
    "    print(f\"   üèóÔ∏è Arquitetura: {architecture}\")\n",
    "    print(f\"   ‚öôÔ∏è Otimiza√ß√£o: {optimization_config}\")\n",
    "    print(f\"   üé® Augmentation: {augmentation_type}\")\n",
    "    print(f\"   üìä Batch size: {batch_size}\")\n",
    "    print(f\"   üîÑ Epochs: {epochs}\")\n",
    "    print(f\"   üéØ Monitor: {CALLBACK_PARAMS['monitor_metric']}\")\n",
    "    \n",
    "    # 5. Treinar modelo\n",
    "    print(\"\\\\nüî• Iniciando treinamento...\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train, Y_train,\n",
    "        validation_data=validation_data,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        callbacks=callbacks,\n",
    "        verbose=verbose\n",
    "    )\n",
    "    \n",
    "    print(\"\\\\n‚úÖ Treinamento conclu√≠do!\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "def quick_train_demo(X_train, Y_train, validation_data=None, epochs=5):\n",
    "    \"\"\"Demonstra√ß√£o r√°pida de treinamento\"\"\"\n",
    "    print(\"‚ö° DEMONSTRA√á√ÉO R√ÅPIDA - 5 √âPOCAS\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    return train_model_modular(\n",
    "        X_train, Y_train,\n",
    "        validation_data=validation_data,\n",
    "        architecture='unet_improved',\n",
    "        optimization_config='advanced',\n",
    "        epochs=epochs,\n",
    "        augmentation_type='none',  # Sem augmentation para rapidez\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "def compare_architectures(X_train, Y_train, validation_data=None, epochs=3):\n",
    "    \"\"\"Comparar diferentes arquiteturas rapidamente\"\"\"\n",
    "    print(\"üèÅ COMPARA√á√ÉO DE ARQUITETURAS\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    results = {}\n",
    "    architectures_to_test = ['unet_base', 'unet_improved']\n",
    "    \n",
    "    for arch in architectures_to_test:\n",
    "        print(f\"\\\\nüß™ Testando: {arch}\")\n",
    "        try:\n",
    "            model, history = train_model_modular(\n",
    "                X_train, Y_train,\n",
    "                validation_data=validation_data,\n",
    "                architecture=arch,\n",
    "                epochs=epochs,\n",
    "                verbose=0\n",
    "            )\n",
    "            \n",
    "            # Pegar m√©tricas finais\n",
    "            final_acc = history.history['val_accuracy'][-1] if 'val_accuracy' in history.history else 0\n",
    "            final_miou = history.history['val_mean_iou_modular'][-1] if 'val_mean_iou_modular' in history.history else 0\n",
    "            \n",
    "            results[arch] = {\n",
    "                'model': model,\n",
    "                'history': history,\n",
    "                'final_accuracy': final_acc,\n",
    "                'final_miou': final_miou\n",
    "            }\n",
    "            \n",
    "            print(f\"‚úÖ {arch}: Acc={final_acc:.3f}, mIoU={final_miou:.3f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erro em {arch}: {str(e)}\")\n",
    "            results[arch] = {'error': str(e)}\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"‚úÖ Sistema de treinamento carregado!\")\n",
    "print(\"üöÄ Fun√ß√µes dispon√≠veis:\")\n",
    "print(\"   ‚Ä¢ train_model_modular() - Treinamento completo\")\n",
    "print(\"   ‚Ä¢ quick_train_demo() - Demo r√°pida\") \n",
    "print(\"   ‚Ä¢ compare_architectures() - Compara√ß√£o autom√°tica\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "8fc71be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé® Carregando sistema de visualiza√ß√£o...\n",
      "‚úÖ Sistema de visualiza√ß√£o carregado!\n",
      "üé® Fun√ß√µes dispon√≠veis:\n",
      "   ‚Ä¢ plot_training_history() - Gr√°ficos de treinamento\n",
      "   ‚Ä¢ predict_and_visualize() - Predi√ß√µes visuais\n",
      "   ‚Ä¢ analyze_class_performance() - An√°lise por classe\n",
      "   ‚Ä¢ compare_models_visually() - Compara√ß√£o visual\n",
      "   ‚Ä¢ create_comprehensive_report() - Relat√≥rio completo\n"
     ]
    }
   ],
   "source": [
    "# üìä M√ìDULO 7: VISUALIZA√á√ÉO E COMPARA√á√ÉO AVAN√áADA\n",
    "print(\"üé® Carregando sistema de visualiza√ß√£o...\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import cv2\n",
    "\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "def plot_training_history(history, title=\"Training History\"):\n",
    "    \"\"\"Plotar hist√≥rico de treinamento completo\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle(title, fontsize=16, fontweight='bold')\n",
    "    \n",
    "    metrics = ['accuracy', 'loss', 'mean_iou_modular', 'learning_rate']\n",
    "    titles = ['Accuracy', 'Loss', 'Mean IoU', 'Learning Rate']\n",
    "    \n",
    "    for i, (metric, title_name) in enumerate(zip(metrics, titles)):\n",
    "        row, col = i // 2, i % 2\n",
    "        ax = axes[row, col]\n",
    "        \n",
    "        if metric in history.history:\n",
    "            epochs = range(1, len(history.history[metric]) + 1)\n",
    "            ax.plot(epochs, history.history[metric], 'b-', label=f'Training {title_name}', linewidth=2)\n",
    "            \n",
    "            val_metric = f'val_{metric}'\n",
    "            if val_metric in history.history:\n",
    "                ax.plot(epochs, history.history[val_metric], 'r-', label=f'Validation {title_name}', linewidth=2)\n",
    "            \n",
    "            ax.set_title(f'{title_name} Progress', fontweight='bold')\n",
    "            ax.set_xlabel('Epoch')\n",
    "            ax.set_ylabel(title_name)\n",
    "            ax.legend()\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Destacar melhor √©poca\n",
    "            if val_metric in history.history:\n",
    "                if 'loss' in metric:\n",
    "                    best_epoch = np.argmin(history.history[val_metric]) + 1\n",
    "                    best_value = min(history.history[val_metric])\n",
    "                else:\n",
    "                    best_epoch = np.argmax(history.history[val_metric]) + 1\n",
    "                    best_value = max(history.history[val_metric])\n",
    "                \n",
    "                ax.axvline(x=best_epoch, color='green', linestyle='--', alpha=0.7)\n",
    "                ax.text(best_epoch, best_value, f'Best: {best_value:.3f}', \n",
    "                       bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"yellow\", alpha=0.7))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def predict_and_visualize(model, X_data, Y_data, num_samples=6, class_names=None):\n",
    "    \"\"\"Fazer predi√ß√µes e visualizar resultados\"\"\"\n",
    "    if class_names is None:\n",
    "        class_names = CONFIG['CLASS_NAMES']\n",
    "    \n",
    "    # Fazer predi√ß√µes\n",
    "    predictions = model.predict(X_data[:num_samples])\n",
    "    pred_classes = np.argmax(predictions, axis=-1)\n",
    "    \n",
    "    # Criar figura\n",
    "    fig, axes = plt.subplots(num_samples, 4, figsize=(16, 4*num_samples))\n",
    "    if num_samples == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        # Imagem original\n",
    "        axes[i, 0].imshow(X_data[i])\n",
    "        axes[i, 0].set_title('Imagem Original', fontweight='bold')\n",
    "        axes[i, 0].axis('off')\n",
    "        \n",
    "        # M√°scara verdadeira\n",
    "        true_mask = Y_data[i]\n",
    "        axes[i, 1].imshow(true_mask, cmap='viridis', vmin=0, vmax=2)\n",
    "        axes[i, 1].set_title('M√°scara Verdadeira', fontweight='bold')\n",
    "        axes[i, 1].axis('off')\n",
    "        \n",
    "        # Predi√ß√£o\n",
    "        pred_mask = pred_classes[i]\n",
    "        axes[i, 2].imshow(pred_mask, cmap='viridis', vmin=0, vmax=2)\n",
    "        axes[i, 2].set_title('Predi√ß√£o', fontweight='bold')\n",
    "        axes[i, 2].axis('off')\n",
    "        \n",
    "        # Sobreposi√ß√£o\n",
    "        overlay = X_data[i].copy()\n",
    "        # Criar m√°scara colorida para sobreposi√ß√£o\n",
    "        colors = [[0, 0, 0], [1, 0, 0], [0, 0, 1]]  # Background, Cat (red), Dog (blue)\n",
    "        colored_pred = np.zeros((*pred_mask.shape, 3))\n",
    "        for c in range(len(colors)):\n",
    "            colored_pred[pred_mask == c] = colors[c]\n",
    "        \n",
    "        # Blend com a imagem original\n",
    "        blended = cv2.addWeighted(overlay.astype(np.float32), 0.7, colored_pred.astype(np.float32), 0.3, 0)\n",
    "        axes[i, 3].imshow(np.clip(blended, 0, 1))\n",
    "        axes[i, 3].set_title('Sobreposi√ß√£o', fontweight='bold')\n",
    "        axes[i, 3].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return predictions, pred_classes\n",
    "\n",
    "def analyze_class_performance(Y_true, Y_pred, class_names=None):\n",
    "    \"\"\"An√°lise detalhada por classe\"\"\"\n",
    "    if class_names is None:\n",
    "        class_names = CONFIG['CLASS_NAMES']\n",
    "    \n",
    "    # Flatten arrays\n",
    "    y_true_flat = Y_true.flatten()\n",
    "    y_pred_flat = Y_pred.flatten()\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_true_flat, y_pred_flat)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=class_names, yticklabels=class_names, ax=axes[0])\n",
    "    axes[0].set_title('Matriz de Confus√£o', fontweight='bold')\n",
    "    axes[0].set_xlabel('Predi√ß√£o')\n",
    "    axes[0].set_ylabel('Verdadeiro')\n",
    "    \n",
    "    # Calculate IoU per class\n",
    "    ious = []\n",
    "    for i in range(len(class_names)):\n",
    "        tp = cm[i, i]\n",
    "        fp = cm[:, i].sum() - tp\n",
    "        fn = cm[i, :].sum() - tp\n",
    "        iou = tp / (tp + fp + fn) if (tp + fp + fn) > 0 else 0\n",
    "        ious.append(iou)\n",
    "    \n",
    "    # Plot IoU per class\n",
    "    bars = axes[1].bar(class_names, ious, color=['gray', 'red', 'blue'], alpha=0.7)\n",
    "    axes[1].set_title('IoU por Classe', fontweight='bold')\n",
    "    axes[1].set_ylabel('IoU')\n",
    "    axes[1].set_ylim(0, 1)\n",
    "    \n",
    "    # Adicionar valores nas barras\n",
    "    for bar, iou in zip(bars, ious):\n",
    "        height = bar.get_height()\n",
    "        axes[1].text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
    "                    f'{iou:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print classification report\n",
    "    print(\"üìä RELAT√ìRIO DE CLASSIFICA√á√ÉO:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(classification_report(y_true_flat, y_pred_flat, target_names=class_names))\n",
    "    \n",
    "    return ious, cm\n",
    "\n",
    "def compare_models_visually(models_results, X_test, Y_test, num_samples=3):\n",
    "    \"\"\"Comparar resultados de m√∫ltiplos modelos visualmente\"\"\"\n",
    "    fig, axes = plt.subplots(num_samples, len(models_results) + 2, \n",
    "                            figsize=(4*(len(models_results) + 2), 4*num_samples))\n",
    "    \n",
    "    if num_samples == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        # Imagem original\n",
    "        axes[i, 0].imshow(X_test[i])\n",
    "        axes[i, 0].set_title('Original', fontweight='bold')\n",
    "        axes[i, 0].axis('off')\n",
    "        \n",
    "        # M√°scara verdadeira\n",
    "        axes[i, 1].imshow(Y_test[i], cmap='viridis', vmin=0, vmax=2)\n",
    "        axes[i, 1].set_title('Ground Truth', fontweight='bold')\n",
    "        axes[i, 1].axis('off')\n",
    "        \n",
    "        # Predi√ß√µes de cada modelo\n",
    "        for j, (model_name, model_data) in enumerate(models_results.items()):\n",
    "            if 'model' in model_data:\n",
    "                model = model_data['model']\n",
    "                pred = model.predict(X_test[i:i+1])\n",
    "                pred_class = np.argmax(pred, axis=-1)[0]\n",
    "                \n",
    "                axes[i, j+2].imshow(pred_class, cmap='viridis', vmin=0, vmax=2)\n",
    "                \n",
    "                # Calcular IoU r√°pido para esta predi√ß√£o\n",
    "                intersection = np.logical_and(Y_test[i], pred_class)\n",
    "                union = np.logical_or(Y_test[i], pred_class)\n",
    "                iou = np.sum(intersection) / np.sum(union) if np.sum(union) > 0 else 0\n",
    "                \n",
    "                axes[i, j+2].set_title(f'{model_name}\\\\nIoU: {iou:.3f}', fontweight='bold')\n",
    "                axes[i, j+2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def create_comprehensive_report(model, history, X_test, Y_test, model_name=\"Modelo\"):\n",
    "    \"\"\"Criar relat√≥rio completo do modelo\"\"\"\n",
    "    print(f\"üìã RELAT√ìRIO COMPLETO: {model_name}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 1. Hist√≥rico de treinamento\n",
    "    print(\"üìà 1. HIST√ìRICO DE TREINAMENTO:\")\n",
    "    plot_training_history(history, f\"{model_name} - Training Progress\")\n",
    "    \n",
    "    # 2. Predi√ß√µes e visualiza√ß√µes\n",
    "    print(\"üé® 2. VISUALIZA√á√ïES DE PREDI√á√ÉO:\")\n",
    "    predictions, pred_classes = predict_and_visualize(model, X_test, Y_test, num_samples=4)\n",
    "    \n",
    "    # 3. An√°lise por classe\n",
    "    print(\"üìä 3. AN√ÅLISE POR CLASSE:\")\n",
    "    ious, cm = analyze_class_performance(Y_test, pred_classes)\n",
    "    \n",
    "    # 4. M√©tricas finais\n",
    "    print(\"üéØ 4. M√âTRICAS FINAIS:\")\n",
    "    final_acc = history.history['val_accuracy'][-1] if 'val_accuracy' in history.history else 0\n",
    "    final_miou = history.history['val_mean_iou_modular'][-1] if 'val_mean_iou_modular' in history.history else 0\n",
    "    \n",
    "    print(f\"   ‚Ä¢ Accuracy Final: {final_acc:.4f} ({final_acc*100:.2f}%)\")\n",
    "    print(f\"   ‚Ä¢ mIoU Final: {final_miou:.4f} ({final_miou*100:.2f}%)\")\n",
    "    print(f\"   ‚Ä¢ IoU Background: {ious[0]:.4f}\")\n",
    "    print(f\"   ‚Ä¢ IoU Cat: {ious[1]:.4f}\")\n",
    "    print(f\"   ‚Ä¢ IoU Dog: {ious[2]:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'final_accuracy': final_acc,\n",
    "        'final_miou': final_miou,\n",
    "        'ious_per_class': ious,\n",
    "        'confusion_matrix': cm,\n",
    "        'predictions': predictions\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Sistema de visualiza√ß√£o carregado!\")\n",
    "print(\"üé® Fun√ß√µes dispon√≠veis:\")\n",
    "print(\"   ‚Ä¢ plot_training_history() - Gr√°ficos de treinamento\")\n",
    "print(\"   ‚Ä¢ predict_and_visualize() - Predi√ß√µes visuais\")\n",
    "print(\"   ‚Ä¢ analyze_class_performance() - An√°lise por classe\")\n",
    "print(\"   ‚Ä¢ compare_models_visually() - Compara√ß√£o visual\")\n",
    "print(\"   ‚Ä¢ create_comprehensive_report() - Relat√≥rio completo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "12539334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ INICIANDO TREINAMENTO MODULAR\n",
      "==================================================\n",
      "üèóÔ∏è Criando modelo: unet_improved\n",
      "‚öôÔ∏è Compilando modelo...\n",
      "‚ö° Criando otimizador: adam\n",
      "   Learning Rate: 0.0001\n",
      "   üéØ Adam: adaptativo, est√°vel, recomendado\n",
      "‚úÖ Modelo compilado!\n",
      "üìä Preparando dados...\n",
      "üîÑ Dados normalizados (0-1)\n",
      "‚úÖ Dados preparados!\n",
      "   üìä X_train: (172, 256, 256, 3)\n",
      "   üìä Y_train: (172, 256, 256)\n",
      "   üìä X_val: (24, 256, 256, 3)\n",
      "   üìä Y_val: (24, 256, 256)\n",
      "‚úÖ Callbacks avan√ßados configurados!\n",
      "üìâ ReduceLR: fator=0.5, paci√™ncia=4\n",
      "üìä CSV Log: training_log_modular.csv\n",
      "üé® Aplicando advanced augmentation...\n",
      "üìà Configura√ß√µes de treinamento:\n",
      "   üèóÔ∏è Arquitetura: unet_improved\n",
      "   ‚öôÔ∏è Otimiza√ß√£o: advanced\n",
      "   üé® Augmentation: advanced\n",
      "   üìä Batch size: 16\n",
      "   üîÑ Epochs: 25\n",
      "   üéØ Monitor: val_mean_iou\n",
      "\\nüî• Iniciando treinamento...\n",
      "--------------------------------------------------\n",
      "‚öôÔ∏è Compilando modelo...\n",
      "‚ö° Criando otimizador: adam\n",
      "   Learning Rate: 0.0001\n",
      "   üéØ Adam: adaptativo, est√°vel, recomendado\n",
      "‚úÖ Modelo compilado!\n",
      "üìä Preparando dados...\n",
      "üîÑ Dados normalizados (0-1)\n",
      "‚úÖ Dados preparados!\n",
      "   üìä X_train: (172, 256, 256, 3)\n",
      "   üìä Y_train: (172, 256, 256)\n",
      "   üìä X_val: (24, 256, 256, 3)\n",
      "   üìä Y_val: (24, 256, 256)\n",
      "‚úÖ Callbacks avan√ßados configurados!\n",
      "üìâ ReduceLR: fator=0.5, paci√™ncia=4\n",
      "üìä CSV Log: training_log_modular.csv\n",
      "üé® Aplicando advanced augmentation...\n",
      "üìà Configura√ß√µes de treinamento:\n",
      "   üèóÔ∏è Arquitetura: unet_improved\n",
      "   ‚öôÔ∏è Otimiza√ß√£o: advanced\n",
      "   üé® Augmentation: advanced\n",
      "   üìä Batch size: 16\n",
      "   üîÑ Epochs: 25\n",
      "   üéØ Monitor: val_mean_iou\n",
      "\\nüî• Iniciando treinamento...\n",
      "--------------------------------------------------\n",
      "Epoch 1/25\n",
      "Epoch 1/25\n",
      "\u001b[1m 1/11\u001b[0m \u001b[32m‚îÅ\u001b[0m\u001b[37m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[1m5:35\u001b[0m 34s/step - accuracy: 0.3382 - loss: 0.9415 - mean_iou_modular: 0.1980"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[383], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Treinamento:\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m model, history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model_modular\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43marchitecture\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43munet_improved\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# ou 'unet_base', 'unet_attention'\u001b[39;49;00m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43madvanced\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[0;32m      6\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[379], line 117\u001b[0m, in \u001b[0;36mtrain_model_modular\u001b[1;34m(X_train, Y_train, validation_data, architecture, optimization_config, epochs, batch_size, augmentation_type, verbose)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mnüî• Iniciando treinamento...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m50\u001b[39m)\n\u001b[1;32m--> 117\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    119\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mn‚úÖ Treinamento conclu√≠do!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m50\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\jonin\\anaconda3\\envs\\trabSeg\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\jonin\\anaconda3\\envs\\trabSeg\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:377\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[0;32m    376\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m--> 377\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    378\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n\u001b[0;32m    379\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[1;32mc:\\Users\\jonin\\anaconda3\\envs\\trabSeg\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:220\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfunction\u001b[39m(iterator):\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m    218\u001b[0m         iterator, (tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mIterator, tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mDistributedIterator)\n\u001b[0;32m    219\u001b[0m     ):\n\u001b[1;32m--> 220\u001b[0m         opt_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs\u001b[38;5;241m.\u001b[39mhas_value():\n\u001b[0;32m    222\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jonin\\anaconda3\\envs\\trabSeg\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\jonin\\anaconda3\\envs\\trabSeg\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\jonin\\anaconda3\\envs\\trabSeg\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[0;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\jonin\\anaconda3\\envs\\trabSeg\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jonin\\anaconda3\\envs\\trabSeg\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\jonin\\anaconda3\\envs\\trabSeg\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32mc:\\Users\\jonin\\anaconda3\\envs\\trabSeg\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\jonin\\anaconda3\\envs\\trabSeg\\lib\\site-packages\\tensorflow\\python\\eager\\context.py:1688\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1686\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1687\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1688\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1689\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1690\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1691\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1692\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1693\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1694\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1695\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1696\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1697\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1698\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1702\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1703\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\jonin\\anaconda3\\envs\\trabSeg\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Treinamento:\n",
    "model, history = train_model_modular(\n",
    "    X_train, Y_train, (X_val, Y_val),\n",
    "    architecture='unet_improved',  # ou 'unet_base', 'unet_attention'\n",
    "    optimization_config='advanced'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f1a3d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068471f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447eb1ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdd5f1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trabSeg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
